{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/hypergraph/blob/main/hgnn_list.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qI0j1J9pwTFg"
      },
      "outputs": [],
      "source": [
        "# @title dgl data\n",
        "!pip install dgl\n",
        "\n",
        "# \"co-cite\" relationship: hyperedge includes all the other papers it cited, as well as the paper itself\n",
        "# then incidence mat = incidence mat + id\n",
        "import torch\n",
        "from dgl.data import CoraGraphDataset # https://github.com/dmlc/dgl/blob/master/python/dgl/data/citation_graph.py\n",
        "\n",
        "def load_data():\n",
        "    dataset = CoraGraphDataset()\n",
        "    graph = dataset[0]\n",
        "    indices = torch.stack(graph.edges())\n",
        "    H = torch.sparse_coo_tensor(indices=indices, values=torch.ones(indices.shape[1]),).coalesce()\n",
        "    id = torch.sparse.spdiags(torch.ones(H.shape[0]),torch.tensor(0),H.shape) # torch.eye(H.shape[0])\n",
        "    H = (id + H).coalesce() # each vert got its hyperedge, contain all cited and itself, [2708, 2708], incedence matrix, |V| hyperedges\n",
        "\n",
        "    X = graph.ndata[\"feat\"] #[2708, 1433] num papers, len bag of words\n",
        "    Y = graph.ndata[\"label\"] # [2708], classiifcation 0-6\n",
        "    train_mask = graph.ndata[\"train_mask\"]\n",
        "    val_mask = graph.ndata[\"val_mask\"]\n",
        "    test_mask = graph.ndata[\"test_mask\"]\n",
        "    return H, X, Y, dataset.num_classes, train_mask, val_mask, test_mask\n",
        "\n",
        "H, X, Y, num_classes, train_mask, val_mask, test_mask = load_data()\n",
        "# print(H.shape, X.shape, Y.shape) # [2708, 2708], [2708, 1433], [2708]\n",
        "\n",
        "n_v, n_e = H.shape\n",
        "elst = [[] for id in range(n_e)] # edge list H(E)={e1,e2,e3}={{A,D},{D,E},{A,B,C}}\n",
        "ilst = [[] for id in range(n_v)] # incidence list {A:{e1,e3}, B:{e3}, C:{e3}, D:{e1,e2}, E:{e2}}\n",
        "for a,b in H.indices().T.tolist():\n",
        "    elst[a].append(b)\n",
        "    ilst[b].append(a)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58WnPtPvT2mx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad\n",
        "def hypergraph_laplacian(H):\n",
        "    N,M = H.shape\n",
        "    d_V = H.sum(1).to_dense() # node deg\n",
        "    d_E = H.sum(0).to_dense() # edge deg\n",
        "    D_v_invsqrt = torch.sparse.spdiags(d_V**-0.5,torch.tensor(0),(N,N)) # torch.diag(d_V**-0.5)\n",
        "    D_e_inv = torch.sparse.spdiags(d_E**-1,torch.tensor(0),(M,M)) # torch.diag(d_E**-1)\n",
        "    B = torch.sparse.spdiags(torch.ones(M),torch.tensor(0),(M,M)) # torch.eye(M) # B is id, dim n_edges\n",
        "    return D_v_invsqrt @ H @ B @ D_e_inv @ H.T @ D_v_invsqrt # Laplacian\n",
        "\n",
        "class HGNN(nn.Module): # https://github.com/dmlc/dgl/blob/master/notebooks/sparse/hgnn.ipynb\n",
        "    def __init__(self, H, in_size, out_size, hidden_dims=16):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(in_size, hidden_dims)\n",
        "        self.W2 = nn.Linear(hidden_dims, out_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.L = hypergraph_laplacian(H)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        X = self.L @ self.W1(self.dropout(X)) # like emb then weighted sum\n",
        "        X = F.relu(X)\n",
        "        X = self.L @ self.W2(self.dropout(X))\n",
        "        return X\n",
        "\n",
        "# Hypergraph Convolution and Hypergraph Attention https://arxiv.org/pdf/1901.08150.pdf\n",
        "class HypergraphAttention(nn.Module): # https://github.com/dmlc/dgl/blob/master/examples/sparse/hypergraphatt.py\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super().__init__()\n",
        "        self.P = nn.Linear(in_size, out_size)\n",
        "        self.a = nn.Linear(2 * out_size, 1)\n",
        "\n",
        "    def forward(self, H, X, X_edges): # H [2708, 2708] n_vert,n_edge ; X n_vert,vembdim\n",
        "        Z = self.P(X) # emb verts [n_vert,out_size]\n",
        "        Z_edges = self.P(X_edges) # emb edges\n",
        "        sim = self.a(torch.cat([Z[H.indices()[0]], Z_edges[H.indices()[1]]], 1)) #  vertemb,edgeemb(=vertemb)\n",
        "        sim = F.leaky_relu(sim, 0.2).squeeze(1) # og[13264]\n",
        "        H_att = torch.sparse_coo_tensor(indices=H.indices(), values=sim,).coalesce()\n",
        "        H_att = torch.sparse.softmax(H_att,1) # [2708, 2708]\n",
        "        return hypergraph_laplacian(H_att) @ Z # [2708, 2708], [2708, hidden_size/out_size]\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_size, out_size, hidden_size=16):\n",
        "        super().__init__()\n",
        "        self.layer1 = HypergraphAttention(in_size, hidden_size)\n",
        "        self.layer2 = HypergraphAttention(hidden_size, out_size)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        Z = self.layer1(H, X, X) # [n_vert, hidden_size]\n",
        "        Z = F.elu(Z)\n",
        "        Z = self.layer2(H, Z, Z) # [n_vert, out_size]\n",
        "        return Z\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsGdDj0kVEjA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title models down\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad\n",
        "def hypergraph_laplacian(H):\n",
        "    N,M = H.shape # num_verts, num_edges\n",
        "    d_V = H.sum(1).to_dense() # node deg\n",
        "    d_E = H.sum(0).to_dense() # edge deg\n",
        "    D_v_invsqrt = torch.sparse.spdiags(d_V**-0.5,torch.tensor(0),(N,N)) # torch.diag(d_V**-0.5)\n",
        "    D_e_inv = torch.sparse.spdiags(d_E**-1,torch.tensor(0),(M,M)) # torch.diag(d_E**-1)\n",
        "    B = torch.sparse.spdiags(torch.ones(M),torch.tensor(0),(M,M)) # torch.eye(M) # B is id, dim n_edges\n",
        "    return D_v_invsqrt @ H @ B @ D_e_inv @ H.T @ D_v_invsqrt # Laplacian\n",
        "\n",
        "class HGNN(nn.Module): # https://github.com/dmlc/dgl/blob/master/notebooks/sparse/hgnn.ipynb\n",
        "    def __init__(self, H, in_size, out_size, hidden_dims=16):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(in_size, hidden_dims)\n",
        "        self.W2 = nn.Linear(hidden_dims, out_size)\n",
        "        self.dropout = nn.Dropout(0.) # og 0.5\n",
        "        self.L = hypergraph_laplacian(H)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        X = self.L @ self.W1(self.dropout(X)) # like emb then weighted sum\n",
        "        X = F.relu(X)\n",
        "        X = self.L @ self.W2(self.dropout(X))\n",
        "        return X\n",
        "\n",
        "# Hypergraph Convolution and Hypergraph Attention https://arxiv.org/pdf/1901.08150.pdf\n",
        "class HypergraphAttention(nn.Module): # https://github.com/dmlc/dgl/blob/master/examples/sparse/hypergraphatt.py\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super().__init__()\n",
        "        self.P = nn.Linear(in_size, out_size)\n",
        "        self.a = nn.Linear(2 * out_size, 1) # og\n",
        "\n",
        "    def forward(self, H, X, X_edges): # H [2708, 2708] n_vert,n_edge ; X n_vert,vembdim\n",
        "        Z = self.P(X) # emb verts [n_vert,out_size]\n",
        "        # Z_edges = self.P(X_edges) # emb edges\n",
        "        # sim = self.a(torch.cat([Z[H.indices()[0]], Z_edges[H.indices()[1]]], 1)) #  vertemb,edgeemb(=vertemb)\n",
        "        sim = self.a(torch.cat([Z[H.indices()[0]], Z[H.indices()[1]]], 1)) #  vertemb,edgeemb(=vertemb)\n",
        "        # sim = F.leaky_relu(sim, 0.2).squeeze(1) # og[13264]\n",
        "        sim = F.relu(sim).squeeze(1) # me\n",
        "        H_att = torch.sparse_coo_tensor(indices=H.indices(), values=sim,).coalesce()\n",
        "        H_att = torch.sparse.softmax(H_att,1) # [2708, 2708]\n",
        "        return hypergraph_laplacian(H_att) @ Z # [2708, 2708], [2708, hidden_size/out_size]\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_size, out_size, hidden_size=16):\n",
        "        super().__init__()\n",
        "        self.layer1 = HypergraphAttention(in_size, hidden_size)\n",
        "        self.layer2 = HypergraphAttention(hidden_size, out_size)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        Z = self.layer1(H, X, X) # [n_vert, hidden_size]\n",
        "        # Z = F.elu(Z) # og\n",
        "        Z = F.relu(Z)\n",
        "        Z = self.layer2(H, Z, Z) # [n_vert, out_size]\n",
        "        return Z\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title theory\n",
        "@torch.no_grad\n",
        "def hypergraph_laplacian(H):\n",
        "    N,M = H.shape # num_verts, num_edges\n",
        "    d_V = H.sum(1).to_dense() # node deg\n",
        "    d_E = H.sum(0).to_dense() # edge deg\n",
        "    D_v_invsqrt = torch.sparse.spdiags(d_V**-0.5,torch.tensor(0),(N,N)) # torch.diag(d_V**-0.5)\n",
        "    D_e_inv = torch.sparse.spdiags(d_E**-1,torch.tensor(0),(M,M)) # torch.diag(d_E**-1)\n",
        "    B = torch.sparse.spdiags(torch.ones(M),torch.tensor(0),(M,M)) # torch.eye(M) # B is id, dim n_edges\n",
        "    return D_v_invsqrt @ H @ B @ D_e_inv @ H.T @ D_v_invsqrt # Laplacian\n",
        "\n",
        "\n",
        "# @torch.no_grad\n",
        "# def hypergraph_laplacian1(H, B):\n",
        "N,M = H.shape # num_verts, num_edges\n",
        "d_V = H.sum(1).to_dense() # node deg\n",
        "d_E = H.sum(0).to_dense() # edge deg\n",
        "D_v_invsqrt = torch.sparse.spdiags(d_V**-0.5,torch.tensor(0),(N,N)) # torch.diag(d_V**-0.5)\n",
        "D_v_inv = torch.sparse.spdiags(d_V**-1,torch.tensor(0),(N,N)) # torch.diag(d_V**-0.5)\n",
        "D_e_inv = torch.sparse.spdiags(d_E**-1,torch.tensor(0),(M,M)) # torch.diag(d_E**-1)\n",
        "D_e_invsqrt = torch.sparse.spdiags(d_E**-0.5,torch.tensor(0),(M,M)) # torch.diag(d_E**-1)\n",
        "B = torch.sparse.spdiags(torch.ones(M),torch.tensor(0),(M,M)) # torch.eye(M) # B is id, dim n_edges\n",
        "# return D_v_invsqrt @ H @ B @ D_e_inv @ H.T @ D_v_invsqrt # Laplacian\n",
        "# return H @ B @ D_e_inv @ H.T @ D_v_inv\n",
        "\n",
        "# hl=hypergraph_laplacian(H)\n",
        "# hl1=hypergraph_laplacian1(H)\n",
        "# print(hl.to_dense()[:5,:5])\n",
        "# print(hl1.to_dense()[:5,:5])\n",
        "\n",
        "n_v, n_e = H.shape\n",
        "d_model=16\n",
        "vemb=torch.rand(n_v,d_model)\n",
        "eemb=torch.rand(n_e,d_model)\n",
        "# B = torch.sparse.spdiags(eemb,torch.tensor(0),(M,M,d_model)) # torch.diag(d_E**-1)\n",
        "\n",
        "\n",
        "                    # [n_vert,n_edge] @ [num_edge, d_model]\n",
        "# vemb1 = (D_v_invsqrt @ H @ B @ D_e_inv @ H.T @ D_v_invsqrt @ self.fv(vemb))\n",
        "out = D_v_invsqrt @ H @ B @ D_e_inv @ H.T @ D_v_invsqrt @ vemb\n",
        "print(out.to_dense()[:5,:5])\n",
        "\n",
        "# # out = D_v_invsqrt @ H @ eemb @ D_e_inv @ H.T @ D_v_invsqrt @ vemb\n",
        "                    # [n_vert,n_edge] @ [num_edge, d_model]\n",
        "vemb1 = D_v_invsqrt @ H @ D_e_invsqrt @ B @ D_e_invsqrt @ H.T @ D_v_invsqrt @ vemb\n",
        "                    # [num_edge,n_edge] @ [n_vert, d_model]\n",
        "eemb1 = D_e_invsqrt @ H.T @ D_v_invsqrt @ V @ D_v_invsqrt @ H @ D_e_invsqrt @ eemb\n",
        "print(out.to_dense()[:5,:5])\n",
        "\n",
        "\n",
        "vmsg = self.fv(vemb) # vmsg = self.fv(torch.cat((vemb, semsg), 1))\n",
        "svmsg = D_e_invsqrt @ H.T @ D_v_invsqrt @ vmsg # [num_edge, d_model]\n",
        "emsg = self.fw(torch.cat((eemb, svmsg), 1))\n",
        "semsg = D_v_invsqrt @ H @ D_e_invsqrt @ emsg\n",
        "\n",
        "vemb1 = self.gv(torch.cat((vemb, semsg), 1))\n",
        "eemb1 = self.gw(torch.cat((eemb, svmsg), 1))\n",
        "\n",
        "\n",
        "vmsg = self.fv(torch.cat((vemb, semsg), 1)) # vmsg = self.fv(vemb)\n",
        "svmsg # H.T @ D_v_invsqrt\n",
        "emsg = self.fw(torch.cat((eemb, svmsg), 1))\n",
        "semsg # D_v_invsqrt @ H @ D_e_inv\n"
      ],
      "metadata": {
        "id": "FrZse0t0OBB1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title adj, inc drop, transform\n",
        "# https://arxiv.org/pdf/2203.16995.pdf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class AdjDropout(nn.Module): # adjacency dropout\n",
        "    def __init__(self, p=0.7):\n",
        "        super(AdjDropout, self).__init__()\n",
        "        self.p=p\n",
        "    def forward(self, H): # randomly remove hyperedges\n",
        "        if self.training: # apply AdjDropout only during training\n",
        "            n_v, n_e = H.shape\n",
        "            mask = (torch.rand(n_e) >= self.p).float().expand(n_v, n_e) # 1->keep, throw p\n",
        "            return H*mask # randomly zero out cols(aka hyperedges)\n",
        "        else: return H\n",
        "\n",
        "class IncDropout(nn.Module):\n",
        "    def __init__(self, p=0.7):\n",
        "        super(IncDropout, self).__init__()\n",
        "        self.p=p\n",
        "    def forward(self, H): # randomly set incidence to 0\n",
        "        if self.training:\n",
        "            Hval = H.values()\n",
        "            mask = (torch.rand(len(Hval)) >= self.p).float() # 1->keep, throw p\n",
        "            return  torch.sparse_coo_tensor(indices=H.indices(), values=Hval*mask,).coalesce() # randomly zero out values(ie remove verts from hyperedges)\n",
        "        else: return H\n",
        "\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = nn.Sequential(IncDropout(0.5), AdjDropout(0.5),)\n",
        "        self.transform_prime = nn.Sequential(IncDropout(0.5), AdjDropout(0.5),)\n",
        "    def __call__(self, H):\n",
        "        return self.transform(H), self.transform_prime(H)\n",
        "trs=TrainTransform()\n",
        "\n",
        "# print(H.to_dense()[:5,:5])\n",
        "# print(trs(H)[1].to_dense()[:5,:5])\n"
      ],
      "metadata": {
        "id": "eUM5WBYX7pRe"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title HMPNN me H attn vic\n",
        "# https://arxiv.org/pdf/2203.16995.pdf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Vert msg = fv(vert ebd) , Sum edge msgs\n",
        "# Edge msg = fw(edge emb, Sum Vert msgs)\n",
        "# Vert emb1 = gv(vert emb, Sum edge msgs)\n",
        "# Edge emb1 = gw(edge emb, Sum Vert msgs)\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    # def __init__(self, d_model, n_heads, dropout=0):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = nn.Linear(d_model, out_dim)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        x = self.drop(attention) @ V # x = torch.matmul(self.drop(attention), V)\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model)\n",
        "        x = self.lin(x)\n",
        "        return x#, attention\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def get_idx(H): # get index of non zero entries for each row\n",
        "    csr=H.to_sparse_csr()\n",
        "    ss=torch.split(csr.col_indices(), tuple(torch.diff(csr.crow_indices()))) # https://stackoverflow.com/a/44536294/13359815\n",
        "    sidx=pad_sequence(ss, batch_first=True, padding_value=-1)\n",
        "    mask=sidx<0\n",
        "    return sidx, mask # [n_rows, num_idx]\n",
        "\n",
        "class ff(nn.Module):\n",
        "    # def __init__(self, in_dim, hid_dim, out_dim):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(ff, self).__init__()\n",
        "        h_dim=16\n",
        "        self.lin = nn.Sequential(\n",
        "            # nn.Linear(in_dim, h_dim), nn.ReLU(), # ReLU Sigmoid\n",
        "            # nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
        "            # nn.Linear(h_dim, out_dim),\n",
        "\n",
        "            # nn.Linear(in_dim, out_dim), nn.BatchNorm1d(out_dim),\n",
        "            # # nn.Sigmoid(), nn.Dropout(p=0.5) # google\n",
        "            # nn.Dropout(p=0.5), nn.Sigmoid() # intra-layer\n",
        "\n",
        "            # nn.BatchNorm1d(in_dim), nn.Linear(in_dim, out_dim), nn.Sigmoid(), nn.Dropout(p=0.1) # me\n",
        "            nn.Linear(in_dim, out_dim), nn.Sigmoid(), nn.Dropout(p=0.5)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "class MsgPass(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(MsgPass, self).__init__()\n",
        "        self.ffv = ff(d_model, d_model)\n",
        "        self.ffw = ff(2*d_model, d_model)\n",
        "        self.fgv = ff(2*d_model, d_model)\n",
        "        self.fgw = ff(2*d_model, d_model)\n",
        "        drop=0.\n",
        "        self.fv = MultiHeadAttention(d_model, n_heads=1, dropout=drop)\n",
        "        self.fw = MultiHeadAttention(d_model, n_heads=1, dropout=drop)\n",
        "        self.gv = MultiHeadAttention(d_model, n_heads=1, dropout=drop)\n",
        "        self.gw = MultiHeadAttention(d_model, n_heads=1, dropout=drop)\n",
        "        # self.adjdrop = AdjDropout(0.7) # 0.7 \"Adjacency dropout must be applied in neighborhood creation steps of Equations 3 through 5\"\n",
        "        self.adjdrop = AdjDropout(0)\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, H, vemb, eemb, emsg=None):\n",
        "        # vmsg = self.ffv(vemb)\n",
        "        # H = AdjDropout(0.7)(H)\n",
        "        # svmsg = self.adjdrop(H).T @ vmsg # sum aggregate\n",
        "        # emsg = self.ffw(torch.cat((eemb, svmsg), 1))\n",
        "        # semsg = self.adjdrop(H) @ emsg\n",
        "        # vemb1 = self.fgv(torch.cat((vemb, semsg), 1))\n",
        "        # svmsg = self.adjdrop(H).T @ vmsg\n",
        "        # eemb1 = self.fgw(torch.cat((eemb, svmsg), 1))\n",
        "\n",
        "        # vemb = self.drop(vemb)\n",
        "\n",
        "        if emsg==None:\n",
        "            vmsg = self.ffv(vemb)\n",
        "        else:\n",
        "            ridx, mask = get_idx(H) # [n_rows, num_idx]\n",
        "            semsg=emsg[ridx] # [n_rows, num_idx, d_model]\n",
        "            mask=mask.unsqueeze(1).unsqueeze(2) # [n_rows, 1, 1, num_idx]\n",
        "            vmsg = self.fv(vemb, semsg, semsg, mask) # [n_rows, 1, d_model]\n",
        "\n",
        "        # vmsg = self.drop(vmsg)\n",
        "        cidx, mask = get_idx(H.T) # [n_cols, num_idx]\n",
        "        svmsg=vmsg[cidx] # [n_cols, num_idx, d_model]\n",
        "        mask=mask.unsqueeze(1).unsqueeze(2) # [n_cols, 1, 1, num_idx]\n",
        "        emsg = self.fw(eemb, svmsg, svmsg, mask) # [n_cols, 1, d_model]\n",
        "\n",
        "        # emsg = self.drop(emsg)\n",
        "        ridx, mask = get_idx(H) # [n_rows, num_idx]\n",
        "        semsg=emsg[ridx] # [n_rows, num_idx, d_model]\n",
        "        mask=mask.unsqueeze(1).unsqueeze(2) # [n_rows, 1, 1, num_idx]\n",
        "        vemb1 = self.gv(vemb, semsg, semsg, mask) # [n_rows, 1, d_model]\n",
        "\n",
        "        # vmsg = self.drop(vmsg)\n",
        "        cidx, mask = get_idx(H.T) # [n_cols, num_idx]\n",
        "        svmsg=vmsg[cidx] # [n_cols, num_idx, d_model]\n",
        "        mask=mask.unsqueeze(1).unsqueeze(2) # [n_cols, 1, 1, num_idx]\n",
        "        eemb1 = self.gw(eemb, svmsg, svmsg, mask) # [n_cols, 1, d_model]\n",
        "        eemb1=eemb1.squeeze()\n",
        "        vemb1=vemb1.squeeze()\n",
        "\n",
        "        return vemb1, eemb1, emsg\n",
        "\n",
        "class HMPNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim):\n",
        "        super(HMPNN, self).__init__()\n",
        "        self.venc = nn.Linear(in_dim, d_model, bias=False)\n",
        "        self.eenc = nn.Linear(in_dim, d_model, bias=False)\n",
        "        self.msgpass = MsgPass(d_model)\n",
        "        self.msgpass2 = MsgPass(d_model)\n",
        "\n",
        "        # f=[d_model,256,256,256]\n",
        "        f=[d_model,32,32,32]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.classifier = nn.Linear(d_model, out_dim)\n",
        "\n",
        "    def forward(self, H, vemb):\n",
        "        # vemb = self.venc(vemb)\n",
        "        vemb, eemb = self.venc(vemb), self.eenc(vemb)\n",
        "        # eemb = torch.zeros(len(elst),self.eembdim)\n",
        "        # eemb = vemb\n",
        "        vemb, eemb, emsg = self.msgpass(H, vemb, eemb)\n",
        "        # vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        vemb, eemb, emsg = self.msgpass2(H, vemb, eemb, emsg=emsg)\n",
        "        # vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        # return vemb\n",
        "        return self.classifier(vemb)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, H1, H2, vemb):\n",
        "        sx, sy = self.forward(H1, vemb), self.forward(H2, vemb)\n",
        "        vx, vy = self.exp(sx), self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def classify(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "num_v,vdim=X.shape\n",
        "# print(\"num_v,vembdim\",num_v,vembdim) # 2708, 1433\n",
        "num_classes=7\n",
        "\n",
        "model=HMPNN(X.shape[1],16,num_classes)\n"
      ],
      "metadata": {
        "id": "WNN62ETxu_Ol",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "2LJiuFC4gEi4"
      },
      "outputs": [],
      "source": [
        "# @title HMPNN me H vicreg\n",
        "# https://arxiv.org/pdf/2203.16995.pdf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Vert msg = fv(vert ebd) , Sum edge msgs\n",
        "# Edge msg = fw(edge emb, Sum Vert msgs)\n",
        "# Vert emb1 = gv(vert emb, Sum edge msgs)\n",
        "# Edge emb1 = gw(edge emb, Sum Vert msgs)\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class ff(nn.Module):\n",
        "    # def __init__(self, in_dim, hid_dim, out_dim):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(ff, self).__init__()\n",
        "        h_dim=out_dim\n",
        "        self.lin = nn.Sequential(\n",
        "            # nn.Linear(in_dim, h_dim), nn.Sigmoid(), #nn.Dropout(p=0.5), # ReLU GELU Sigmoid\n",
        "            # nn.Linear(h_dim, h_dim), nn.Sigmoid(), #nn.Dropout(p=0.5),\n",
        "            # nn.Linear(h_dim, out_dim), nn.Sigmoid()\n",
        "\n",
        "            # nn.BatchNorm1d(in_dim), nn.Linear(in_dim, out_dim), nn.Sigmoid(), #nn.Dropout(p=0.1) # me\n",
        "            # nn.Linear(in_dim, out_dim), nn.Sigmoid(), nn.Dropout(p=0.5) # best nah\n",
        "            nn.Linear(in_dim, out_dim), nn.Sigmoid()#, nn.Dropout(p=0.5) #\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "class MsgPass(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(MsgPass, self).__init__()\n",
        "        self.fv = ff(2*d_model, d_model)\n",
        "        self.fw = ff(2*d_model, d_model)\n",
        "        self.gv = ff(2*d_model, d_model)\n",
        "        self.gw = ff(2*d_model, d_model)\n",
        "        # self.adjdrop = AdjDropout(0.3) # 0.7 \"Adjacency dropout must be applied in neighborhood creation steps of Equations 3 through 5\"\n",
        "        self.adjdrop = AdjDropout(0)\n",
        "\n",
        "    def forward(self, H, vemb, eemb, semsg=None):\n",
        "        N,M = H.shape\n",
        "        d_V = H.sum(1).to_dense() # node deg\n",
        "        d_E = H.sum(0).to_dense() # edge deg\n",
        "        d_V[d_V==0] = float('inf')\n",
        "        d_E[d_E==0] = float('inf')\n",
        "        D_v_invsqrt = torch.sparse.spdiags(d_V**-0.5,torch.tensor(0),(N,N))\n",
        "        D_e_invsqrt = torch.sparse.spdiags(d_E**-0.5,torch.tensor(0),(M,M))\n",
        "\n",
        "        # H = AdjDropout(0.7)(H)\n",
        "\n",
        "        if semsg == None: semsg = vemb\n",
        "        vmsg = vemb + self.fv(torch.cat((vemb, semsg), 1))\n",
        "        svmsg = D_e_invsqrt @ self.adjdrop(H).T @ D_v_invsqrt @ vmsg # [num_edge, d_model]\n",
        "        emsg = svmsg + self.fw(torch.cat((eemb, svmsg), 1))\n",
        "        semsg = D_v_invsqrt @ self.adjdrop(H) @ D_e_invsqrt @ emsg\n",
        "\n",
        "        vemb1 = semsg + self.gv(torch.cat((vemb, semsg), 1))\n",
        "        eemb1 = svmsg + self.gw(torch.cat((eemb, svmsg), 1))\n",
        "        return vemb1, eemb1, semsg\n",
        "\n",
        "class HMPNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim):\n",
        "        super(HMPNN, self).__init__()\n",
        "        self.venc = nn.Linear(in_dim, d_model)\n",
        "        self.eenc = nn.Linear(in_dim, d_model)\n",
        "        self.msgpass = MsgPass(d_model)\n",
        "        self.msgpass2 = MsgPass(d_model)\n",
        "        self.msgpass3 = MsgPass(d_model)\n",
        "        f=[d_model,256,256,256]\n",
        "        # f=[d_model,32,32,32]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.classifier = nn.Linear(d_model, out_dim)\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_normal_(p) # xavier_uniform_ xavier_normal_\n",
        "\n",
        "    def forward(self, H, X, classify=True):\n",
        "        vemb, eemb = self.venc(X), self.eenc(X)\n",
        "        vemb1, eemb1, semsg = self.msgpass(H, vemb, eemb)\n",
        "        vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        vemb1, eemb1, semsg = self.msgpass2(H, vemb, eemb, semsg=semsg)\n",
        "        vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        vemb1, eemb1, semsg = self.msgpass3(H, vemb, eemb, semsg=semsg)\n",
        "        vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        if classify==False: return vemb\n",
        "        else: return self.classifier(vemb)\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size, num_features = x.shape\n",
        "        sim_coeff=5.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, H1, H2, X):\n",
        "        sx, sy = self.forward(H1, X, classify=False), self.forward(H2, X, classify=False)\n",
        "        vx, vy = self.exp(sx), self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def classify(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "num_v,in_dim=X.shape # 2708, 1433\n",
        "num_classes=7\n",
        "model=HMPNN(X.shape[1],16,num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "eloGsSge3YI6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title HMPNN me H\n",
        "# https://arxiv.org/pdf/2203.16995.pdf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Vert msg = fv(vert ebd) , Sum edge msgs\n",
        "# Edge msg = fw(edge emb, Sum Vert msgs)\n",
        "# Vert emb1 = gv(vert emb, Sum edge msgs)\n",
        "# Edge emb1 = gw(edge emb, Sum Vert msgs)\n",
        "\n",
        "class ff(nn.Module):\n",
        "    # def __init__(self, in_dim, hid_dim, out_dim):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(ff, self).__init__()\n",
        "        h_dim=out_dim\n",
        "        self.lin = nn.Sequential(\n",
        "            # nn.Linear(in_dim, h_dim), nn.Sigmoid(), #nn.Dropout(p=0.5), # ReLU GELU Sigmoid\n",
        "            # nn.Linear(h_dim, h_dim), nn.Sigmoid(), #nn.Dropout(p=0.5),\n",
        "            # nn.Linear(h_dim, out_dim), nn.Sigmoid()\n",
        "\n",
        "            # nn.BatchNorm1d(in_dim), nn.Linear(in_dim, out_dim), nn.Sigmoid(), #nn.Dropout(p=0.1) # me\n",
        "            # nn.Linear(in_dim, out_dim), nn.Sigmoid(), nn.Dropout(p=0.5) # best nah\n",
        "            nn.Linear(in_dim, out_dim), nn.Sigmoid()#, nn.Dropout(p=0.5) #\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "class MsgPass(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(MsgPass, self).__init__()\n",
        "        # self.ffv = ff(d_model, d_model)\n",
        "        self.fv = ff(2*d_model, d_model)\n",
        "        self.fw = ff(2*d_model, d_model)\n",
        "        self.gv = ff(2*d_model, d_model)\n",
        "        self.gw = ff(2*d_model, d_model)\n",
        "        # self.adjdrop = AdjDropout(0.3) # 0.7 \"Adjacency dropout must be applied in neighborhood creation steps of Equations 3 through 5\"\n",
        "        self.adjdrop = AdjDropout(0)\n",
        "\n",
        "    def forward(self, H, vemb, eemb, semsg=None):\n",
        "        N,M = H.shape\n",
        "        d_V = H.sum(1).to_dense() # node deg\n",
        "        d_E = H.sum(0).to_dense() # edge deg\n",
        "        D_v_invsqrt = torch.sparse.spdiags(d_V**-0.5,torch.tensor(0),(N,N))\n",
        "        D_e_invsqrt = torch.sparse.spdiags(d_E**-0.5,torch.tensor(0),(M,M))\n",
        "        # D_v_inv = torch.sparse.spdiags(d_V**-1,torch.tensor(0),(N,N))\n",
        "        # D_e_inv = torch.sparse.spdiags(d_E**-1,torch.tensor(0),(M,M))\n",
        "\n",
        "        # H = AdjDropout(0.7)(H)\n",
        "\n",
        "        # if semsg != None: vmsg = vemb + self.fv(torch.cat((vemb, semsg), 1))\n",
        "        # else: vmsg = vemb + self.ffv(vemb)\n",
        "\n",
        "        # if semsg == None: semsg = torch.zeros(eemb.shape)\n",
        "        if semsg == None: semsg = vemb\n",
        "        vmsg = vemb + self.fv(torch.cat((vemb, semsg), 1))\n",
        "        svmsg = D_e_invsqrt @ self.adjdrop(H).T @ D_v_invsqrt @ vmsg # [num_edge, d_model]\n",
        "        emsg = svmsg + self.fw(torch.cat((eemb, svmsg), 1))\n",
        "        semsg = D_v_invsqrt @ self.adjdrop(H) @ D_e_invsqrt @ emsg\n",
        "\n",
        "        vemb1 = semsg + self.gv(torch.cat((vemb, semsg), 1))\n",
        "        eemb1 = svmsg + self.gw(torch.cat((eemb, svmsg), 1))\n",
        "        return vemb1, eemb1, semsg\n",
        "\n",
        "class HMPNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim):\n",
        "        super(HMPNN, self).__init__()\n",
        "        self.venc = nn.Linear(in_dim, d_model)\n",
        "        self.eenc = nn.Linear(in_dim, d_model)\n",
        "        self.msgpass = MsgPass(d_model)\n",
        "        self.msgpass2 = MsgPass(d_model)\n",
        "        self.msgpass3 = MsgPass(d_model)\n",
        "        self.classifier = nn.Linear(d_model, out_dim)\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_normal_(p) # xavier_uniform_ xavier_normal_\n",
        "\n",
        "    def forward(self, H, vemb):\n",
        "        vemb, eemb = self.venc(vemb), self.eenc(vemb)\n",
        "        # eemb = torch.zeros(len(elst),self.eembdim)\n",
        "        vemb1, eemb1, semsg = self.msgpass(H, vemb, eemb)\n",
        "        vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        vemb1, eemb1, semsg = self.msgpass2(H, vemb, eemb, semsg=semsg)\n",
        "        vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        # vemb1, eemb1, semsg = self.msgpass3(H, vemb, eemb, semsg=semsg)\n",
        "        # vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        return self.classifier(vemb)\n",
        "\n",
        "\n",
        "num_v,in_dim=X.shape # 2708, 1433\n",
        "num_classes=7\n",
        "model=HMPNN(X.shape[1],16,num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "IfEc6JRXwHPt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title train/ eval\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train(model, optimizer, H, X, Y, train_mask):\n",
        "    model.train()\n",
        "    Y_hat = model(H, X)\n",
        "    loss = F.cross_entropy(Y_hat[train_mask], Y[train_mask]) # loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "#     print(\"test acc: \",accuracy(Y_hat[train_mask].argmax(1), Y[train_mask]))\n",
        "    return loss.item()\n",
        "\n",
        "def victrain(model, optimizer, H, X):\n",
        "    model.train()\n",
        "    # H1, H2 = trs(H)\n",
        "    H1, H2 = H,H\n",
        "    loss = model.loss(H1, H2, X)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def accuracy(yhat, y): return 100*(yhat == y).type(torch.float).sum().item()/y.shape[0]\n",
        "\n",
        "def evaluate(model, H, X, Y, val_mask, test_mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Y_hat = model(H, X) # model(X)\n",
        "    # print(Y_hat[val_mask].shape, Y[val_mask].shape)\n",
        "    # print(Y_hat[val_mask], Y[val_mask])\n",
        "    val_acc = accuracy(Y_hat[val_mask].argmax(1), Y[val_mask])\n",
        "    test_acc = accuracy(Y_hat[test_mask].argmax(1), Y[test_mask])\n",
        "    return val_acc, test_acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m-1wJk4LZFg"
      },
      "outputs": [],
      "source": [
        "# @title run\n",
        "\n",
        "\n",
        "# model = HGNN(H, X.shape[1], num_classes) # hg conv\n",
        "# model = Net(X.shape[1], num_classes) # hg att\n",
        "# model = HMPNN(num_classes, vembdim, eembdim) # hg msg pass\n",
        "# model = Network(in_channels=X.shape[1], hidden_channels=8, out_channels=num_classes, n_layers=2, task_level=\"node\")\n",
        "# model=HMPNN(num_classes, vembdim, eembdim, vmsgdim, emsgdim)\n",
        "# model=HMPNN(X.shape[1],16,num_classes)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, betas=(0.9, 0.999), eps=1e-08, weight_decay=3e-6) # vicreg1e-4\n",
        "# coptimizer = torch.optim.AdamW(model.parameters(), lr=3e-2) # vicreg1e-3\n",
        "# coptimizer = torch.optim.AdamW(model.classifier.parameters(), lr=3e-2) # vicreg1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # 0.001 # og\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # 0.001 # og\n",
        "# optimizer.param_groups[0]['lr']=0.01\n",
        "\n",
        "for epoch in range(200):\n",
        "    loss = train(model, optimizer, H, X, Y, train_mask)\n",
        "    for _ in range(7):\n",
        "        loss = victrain(model, optimizer, H, X)\n",
        "    val_acc, test_acc = evaluate(model, H, X, Y, val_mask, test_mask)\n",
        "\n",
        "    # loss = ctrain(model, optimizer, H, X, Y, train_mask)\n",
        "    # val_acc, test_acc = vicevaluate(model, H, X, Y, val_mask, test_mask)\n",
        "    print(f\"{epoch+1} test loss: {loss:.5f}, Val acc: {val_acc:.2f}, Test acc: {test_acc:.2f}\")\n",
        "\n",
        "end = time.time()\n",
        "print(\"time: \",end - start)\n",
        "\n",
        "\n",
        "# dhg\n",
        "# HGNN 200epoch 14sec test loss: 0.15217, Val acc: 0.79600, Test acc: 0.79600\n",
        "# HGNN drop0 200 test loss: 0.06214, Val acc: 78.00, Test acc: 79.50\n",
        "\n",
        "# attn lr=0.01 200epoch 27 sec test loss: 0.02389, Val acc: 0.77400, Test acc: 0.78900\n",
        "# attn Ponce 200 test loss: 0.04812, Val acc: 77.20, Test acc: 76.70\n",
        "\n",
        "# hmpnn relu 200 epoch test loss: 0.00001, Val acc: 0.53600, Test acc: 0.51900\n",
        "# hmpnn sigmoid 200 epoch test loss: 0.00885, Val acc: 0.55000, Test acc: 0.52200\n",
        "# 2hmpnn sigmoid 200 epoch test loss: 1.94591, Val acc: 0.31600, Test acc: 0.31900  Val acc: 0.11400, Test acc: 0.10300\n",
        "# 2hmpnn 2lin sigmoid 200 epoch test loss: 1.94591, Val acc: 0.11400, Test acc: 0.10300\n",
        "# 2hmpnn 2lin sigmoid noadjdrop 200 epoch test loss: 1.94591, Val acc: 0.16200, Test acc: 0.14900 Val acc: 0.05800, Test acc: 0.06400\n",
        "# 2hmpnn 2lin sigmoid noadjdrop res 200 epoch test loss: 0.01424, Val acc: 0.37400, Test acc: 0.39700\n",
        "# 2hmpnn 2lin sigmoid noadjdrop nodrop res 200 epoch 5m47s test loss: 0.08539, Val acc: 0.31000, Test acc: 0.36100\n",
        "# 2hmpnn sigmoid noadjdrop nodrop res 200 epoch 11m6s test loss: 0.00635, Val acc: 0.55200, Test acc: 0.53700\n",
        "# 3hmpnn sigmoid noadjdrop nodrop res 200 epoch test loss: 0.00638, Val acc: 0.54200, Test acc: 0.52900\n",
        "\n",
        "# request\n",
        "# HGNN 200epoch 12sec test loss: 0.15115, Val acc: 0.26200, Test acc: 0.23200\n",
        "# attn 200epoch test loss: 0.00675, Val acc: 0.25000, Test acc: 0.22800\n",
        "# hmpnn relu 7 epoch test loss: 0.00779, Val acc: 0.53800, Test acc: 0.47500\n",
        "# hmpnn relu 200 epoch test loss: 0.00000, Val acc: 0.43400, Test acc: 0.41000\n",
        "# hmpnn 2lin relu 200 epoch test loss: 0.02837, Val acc: 0.44000, Test acc: 0.38000\n",
        "# hmpnn sigmoid test loss: 0.00064, Val acc: 0.53000, Test acc: 0.45100\n",
        "# 2hmpnn sigmoid test loss: 1.81603, Val acc: 0.35000, Test acc: 0.29500\n",
        "# 2hmpnn relu test loss: 0.00000, Val acc: 0.42800, Test acc: 0.45100\n",
        "# 2hmpnn 2lin relu 200 epoch test loss: 1.81581, Val acc: 0.35000, Test acc: 0.29500\n",
        "# 2hmpnn 2lin sigmoid 200 epoch test loss: 1.81585, Val acc: 0.35000, Test acc: 0.29500\n",
        "\n",
        "# @ 3hpnn 1lin relu 400 test loss: 0.04027, Val acc: 0.28, Test acc: 0.28\n",
        "# @ 3hpnn 2lin relu 200 test loss: 0.09888, Val acc: 0.34, Test acc: 0.36 400 test loss: 0.00260, Val acc: 0.42, Test acc: 0.40\n",
        "# @ 3hpnn 2lin sigmoid 200 test loss: 0.45359, Val acc: 31.40, Test acc: 28.40\n",
        "# @ 2hpnn 2lin sigmoid 200 test loss: 0.20277, Val acc: 32.20, Test acc: 32.00\n",
        "# @ hpnn 2lin sigmoid 200 test loss: 0.09099, Val acc: 42.20, Test acc: 41.50\n",
        "# @ hpnn 2lin relu 200 test loss: 0.00447, Val acc: 33.60, Test acc: 38.80\n",
        "# @ hpnn lin relu 200 test loss: 0.15811, Val acc: 33.80, Test acc: 33.50; 400 test loss: 0.01975, Val acc: 36.40, Test acc: 35.00\n",
        "# @ hpnn lin sigmoid 200 test loss: 0.47640, Val acc: 33.80, Test acc: 33.70; 400test loss: 0.09173, Val acc: 33.60, Test acc: 34.50\n",
        "# @ hpnn 2lin relu hdim8 200 test loss: 0.00264, Val acc: 46.80, Test acc: 46.70\n",
        "# @ 3hpnn 2lin relu hdim8 200 test loss: 0.00477, Val acc: 28.00, Test acc: 27.10; 400 test loss: 0.00045, Val acc: 31.00, Test acc: 29.80\n",
        "# @ hpnn3 2lin relu hdim8 200 test loss: 0.00310, Val acc: 49.20, Test acc: 50.00\n",
        "# @ hpnn5 2lin relu hdim8 200 test loss: 0.00337, Val acc: 46.20, Test acc: 46.70\n",
        "\n",
        "# @ hpnn2 2lin relu hdim2 drop0.5 adjdrop0.7 200 test loss: 0.53219, Val acc: 36.00, Test acc: 38.80 ; test loss: 0.21715, Val acc: 38.20, Test acc: 40.40 ;\n",
        "# @ hpnn2 2lin relu hdim2 adjdrop 200 test loss: 0.01526, Val acc: 33.20, Test acc: 32.70\n",
        "# @ hpnn2 2lin relu hdim2 adjdrop drop 200 test loss: 0.36543, Val acc: 30.40, Test acc: 35.20 ; test loss: 0.18227, Val acc: 32.80, Test acc: 36.20\n",
        "# @ hpnn2 2lin sig hdim2 adjdrop drop 200 test loss: 0.28160, Val acc: 31.60, Test acc: 34.60 ; test loss: 0.10077, Val acc: 33.80, Test acc: 38.00\n",
        "# @ hpnn2 lin sig hdim2 adjdrop drop 200 test loss: 0.77846, Val acc: 28.20, Test acc: 30.00 ; test loss: 0.17251, Val acc: 32.20, Test acc: 33.20\n",
        "# @ hpnn2 lin sig hdim16 adjdrop drop 200 test loss: 0.00632, Val acc: 50.00, Test acc: 50.70\n",
        "# @ hpnn2 lin sig hdim16 drop adjdrop 200 test loss: 0.00569, Val acc: 55.20, Test acc: 57.70\n",
        "# hpnn2 lin sig hdim16 drop0.5 noadjdrop eemb=eenc(vemb) res12 100 test loss: 0.77957, Val acc: 64.40 starts decreasing, Test acc: 63.20\n",
        "# hpnn2 lin sig hdim16 drop0.5 adjdrop0.7123 eemb=eenc(vemb) res12 200 test loss: 0.38732, Val acc: 64.00 starts decreasing, Test acc: 62.50\n",
        "# hpnn2 lin sig hdim16 drop0.5 adjdrop0.7 eemb=eenc(vemb) res12 200 test loss: 0.39054, Val acc: 60.40, Test acc: 57.50\n",
        "# follow noadjdrop 200 test loss: 0.00675, Val acc: 63.60, Test acc: 62.60\n",
        "# follow noadjdrop normalise 200 test loss: 0.00582, Val acc: 63.40, Test acc: 62.60\n",
        "# follow linbiasF noadjdrop normalise 200 test loss: 0.00923, Val acc: 61.80, Test acc: 58.70\n",
        "# res1 nope\n",
        "# hpnn3 res123 follow noadjdrop normalise 800? test loss: 0.00241, Val acc: 65.20, Test acc: 63.20\n",
        "# hpnn2 res12 follow noadjdrop 200 test loss: 0.00748, Val acc: 65.60, Test acc: 65.20\n",
        "# hpnn2 almostcopy noadjdrop 200 test loss: 0.00376, Val acc: 72.40, Test acc: 74.80\n",
        "\n",
        "# @ hpnn2 lin aggsig hdim16 drop adjdrop 200 test loss: 0.00634, Val acc: 54.80, Test acc: 56.70\n",
        "# nores test loss: 0.10421, Val acc: 44.00, Test acc: 42.50\n",
        "# @ hpnn2 lin sigagg hdim16 drop adjdrop 200 test loss: 0.00952, Val acc: 48.00, Test acc: 47.20\n",
        "\n",
        "# @ hpnn2 lin sigagg hdim16 drop adjdrop relu vembX 100 test loss: 0.00001, Val acc: 50.00, Test acc: 50.20\n",
        "# @ hpnn2 lin sigagg hdim16 drop adjdrop sig vembX 100 test loss: 0.00071, Val acc: 52.20, Test acc: 50.40\n",
        "\n",
        "#\n",
        "# @ hpnn2 lin ggBDLS ffLBDS hdim16 drop adjdrop345 100 test loss: 0.01240, Val acc: 45.60, Test acc: 44.60\n",
        "# @ hpnn2 lin ggBDLS ffLBDS hdim16 drop adjdrop 100 test loss: 0.03899, Val acc: 40.60, Test acc: 42.50\n",
        "# @ hpnn2 lin ggffBLSD hdim16 drop adjdrop 100 test loss: 0.00002, Val acc: 28.80, Test acc: 31.10\n",
        "# @ hpnn2 lin ggffBLSD hdim16 drop0.1 adjdrop0 100 test loss: 0.00000, Val acc: 30.60, Test acc: 31.30\n",
        "# @ hpnn2 lin hdim16 drop0.1 adjdrop0 100 test loss: 1.87673, Val acc: 12.80, Test acc: 12.10\n",
        "# @ hpnn2 lin hdim16 drop0.1 adjdrop0.7 100 test loss: 0.92645, Val acc: 36.60, Test acc: 37.60\n",
        "# @ hpnn2 lin hdim16 drop0.5 adjdrop0.7 100 test loss: 1.09457, Val acc: 42.00, Test acc: 37.10\n",
        "# adj train~40\n",
        "\n",
        "# 100 test loss: 0.00412, Val acc: 36.60, Test acc: 39.30\n",
        "\n",
        "# 10,10,1 noadjdrop 1000 test loss: 6.19388, Val acc: 24.20, Test acc: 25.90\n",
        "# 15,15,1 trs 0.3 1000 test loss: 0.00289, Val acc: 43.40, Test acc: 39.30\n",
        "# 15,15,1 trs 0.5 137 test loss: 0.06801, Val acc: 35.00, Test acc: 41.60\n",
        "# 15,15,1 trs 0.7 357 test loss: 0.00001, Val acc: 28.60, Test acc: 31.00\n",
        "# 15,15,1 trs 0.1 154 test loss: 0.08117, Val acc: 34.40, Test acc: 35.60\n",
        "\n",
        "# eemb=vemb 1000 test loss: 0.07657, Val acc: 50.80, Test acc: 54.90\n",
        "\n",
        "# mha d_model2 828 test loss: 0.00944, Val acc: 27.20, Test acc: 24.90\n",
        "# mha d_model2 vicreg 26 test loss: 1.94425, Val acc: 7.40, Test acc: 9.90\n",
        "# mha d_model16 vicreg exp32 54 test loss: 0.00315, Val acc: 48.40, Test acc: 47.30 in vicreg  0.0 0.06320717930793762 1.4348915815353394\n",
        "# mha d_model16 96 test loss: 0.03089, Val acc: 48.80, Test acc: 48.70 # 276 test loss: 0.00349, Val acc: 47.60, Test acc: 49.20\n",
        "\n",
        "# mha d_model16 vicreg10,10,1 exp32 2708, 169, 1433\n",
        "# mha d_model16 nores2 158 test loss: 0.00906, Val acc: 50.20, Test acc: 50.70\n",
        "# mha d_model16 nores 219 test loss: 0.00544, Val acc: 49.60, Test acc: 49.60\n",
        "# mha d_model16 nores embbiasT 112 test loss: 0.02248, Val acc: 50.20, Test acc: 48.70\n",
        "\n",
        "# mha d_model128 nores 18 test loss: 0.45603, Val acc: 51.80, Test acc: 51.20 37 test loss: 0.00388, Val acc: 50.80, Test acc: 51.40\n",
        "\n",
        "# mha drop0.5 adam1e-2 101 test loss: 0.02396, Val acc: 51.80, Test acc: 50.00\n",
        "# mhadrop0.5 178 test loss: 1.94971, Val acc: 31.60, Test acc: 31.90\n",
        "\n",
        "# 2lin drop0.5 adjdrop0.7123 200 test loss: 1.05092, Val acc: 17.80, Test acc: 20.70\n",
        "# 2lin drop0.5 adjdrop0.7 200 test loss: 0.95048, Val acc: 33.80, Test acc: 28.60\n",
        "# 2lin gelu drop0.5 adjdrop0.7 200 test loss: 1.03655, Val acc: 19.60, Test acc: 21.10\n",
        "# 2lin sig drop0.5 adjdrop0.7 200 test loss: 1.43882, Val acc: 16.40 huge variation, Test acc: 17.10\n",
        "\n",
        "# lin sig 200 drop0.5 adjdrop0.7123 test loss: 1.86198, Val acc: 56.60, Test acc: 60.90\n",
        "# better than batchnorm ,adjdrop0.7, 2lin\n",
        "# 3lin sig nodrop noadjdrop 600? test loss: 0.08037, Val acc: 26.00, Test acc: 27.30\n",
        "\n",
        "# # 3lin sig nodrop noadjdrop in vicreg  0.08287973701953888 0.6470489501953125 1.871908187866211\n",
        "# test acc:  100.0\n",
        "# 591 test loss: 0.00912, Val acc: 15.80, Test acc: 17.20\n",
        "# 5,5,1in vicreg  0.007232982665300369 0.007199370302259922 0.4727526307106018\n",
        "# test acc:  100.0\n",
        "# 100 test loss: 0.00160, Val acc: 20.00, Test acc: 19.40\n",
        "\n",
        "# lin D_e_inv@emsg 100 test loss: 2.38527, Val acc: 36.80, Test acc: 33.40\n",
        "\n",
        "# res 200 test loss: 0.00703, Val acc: 57.60, Test acc: 54.30\n",
        "# resfgvw 200 test loss: 0.00358, Val acc: 72.60, Test acc: 74.80\n",
        "# resfgvw encbiasT 200 test loss: 0.00470, Val acc: 76.40, Test acc: 76.10\n",
        "# resfgvw encbiasT nodrop 200 test loss: 0.00243, Val acc: 77.40, Test acc: 75.40\n",
        "# resfgvw semsg=zeros encbiasT nodrop 200 test loss: 0.00329, Val acc: 76.80, Test acc: 76.50\n",
        "# resfgvw semsg=zeros encbiasT nodrop adjdrop0.7 nope 50+?\n",
        "# resfgvw semsg=zeros encbiasT nodrop adjdrop0.712 200 test loss: 0.00292, Val acc: 71.20, Test acc: 74.30\n",
        "# resfgvw semsg=zeros encbiasT nodrop adjdrop0.312 200 test loss: 0.00302, Val acc: 74.60, Test acc: 77.30\n",
        "# resfgvw 2lin semsg=zeros encbiasT nodrop noadjdrop 200 test loss: 0.00020, Val acc: 74.20, Test acc: 74.80\n",
        "# resfgvw hmpnn3 1lin semsg=zeros encbiasT nodrop noadjdrop 200 test loss: 0.00543, Val acc: 76.80, Test acc: 77.70 79at49epochs\n",
        "# inv 200 test loss: 0.00271, Val acc: 54.00, Test acc: 54.40\n",
        "\n",
        "# resfgvw ls semsg=vemb 200 test loss: 0.00239, Val acc: 78.00, Test acc: 75.80 again 200 test loss: 0.00183, Val acc: 76.60, Test acc: 77.10\n",
        "# lbsd 200 test loss: 0.00846, Val acc: 67.00, Test acc: 65.60\n",
        "# lbds 200 test loss: 0.00780, Val acc: 67.00, Test acc: 68.30\n",
        "# lds 200 test loss: 0.00686, Val acc: 74.00, Test acc: 73.60\n",
        "# dls 200 test loss: 0.00442, Val acc: 76.60, Test acc: 77.90\n",
        "# lsd 200 test loss: 0.00501, Val acc: 76.20, Test acc: 75.00\n",
        "# bls 200 test loss: 0.00167, Val acc: 67.20, Test acc: 68.10\n",
        "# lsl 200 test loss: 0.00024, Val acc: 73.40, Test acc: 74.30\n",
        "# lsl hdim*2 200 test loss: 0.00059, Val acc: 74.20, Test acc: 75.90\n",
        "# lsls hdim*1 200 test loss: 0.00447, Val acc: 76.80, Test acc: 77.80\n",
        "\n",
        "# xavier_uniform 200 test loss: 0.00451, Val acc: 74.00, Test acc: 74.10 200 test loss: 0.00697, Val acc: 75.80, Test acc: 77.00 200 test loss: 0.00655, Val acc: 77.20, Test acc: 77.10\n",
        "# xavier_normal 200 test loss: 0.00638, Val acc: 77.20, Test acc: 77.40 200 test loss: 0.00653, Val acc: 77.60, Test acc: 78.90 200 test loss: 0.00394, Val acc: 75.80, Test acc: 76.10\n",
        "# xavier_normal semsg=zeros 200 test loss: 0.00778, Val acc: 77.40, Test acc: 77.90 200 test loss: 0.00777, Val acc: 74.20, Test acc: 75.90\n",
        "# xavier_normal semsg=vemb 200 test loss: 0.00669, Val acc: 78.00, Test acc: 77.30 200 test loss: 0.00502, Val acc: 75.20, Test acc: 75.70\n",
        "# lr\n",
        "# lg\n",
        "# le\n",
        "# ll-r\n",
        "\n",
        "\n",
        "# 1vic10,10,1 200 test loss: 0.42472, Val acc: 70.80, Test acc: 74.00\n",
        "# 1vic5,10,1 200 test loss: 0.40446, Val acc: 74.40, Test acc: 75.50 # in vicreg  0.0 0.018776636570692062 0.3856853246688843\n",
        "# 7vic5,10,1 trs0.1 153 test loss: 0.00000, Val acc: 75.20, Test acc: 75.40 #in vicreg  0.0 0.0 1.8084348596403288e-07\n",
        "# 7vic5,10,1 trs0.3 200 test loss: 0.00000, Val acc: 72.20, Test acc: 77.00 # in vicreg  0.0 0.0 1.730809202626915e-07\n",
        "# 7vic5,10,1 trs0.5 200 test loss: 0.00000, Val acc: 71.40, Test acc: 74.10 # in vicreg  0.0 0.0 5.946303360815364e-08\n",
        "# hpnn3 7vic5,10,1 trs0.5 200 test loss: 0.00000, Val acc: 78.60, Test acc: 79.90 # in vicreg  0.0 0.0 1.24092821351951e-07 # in vicreg  0.0 0.0 6.852277e-08 200 test loss: 0.00000, Val acc: 74.60, Test acc: 76.10\n",
        "# hpnn3 7vic5,10,1 trs0.5 exp256 200 test loss: 6.25101, Val acc: 72.00, Test acc: 72.60 # in vicreg  0.0 5.1014814376831055 1.1495264768600464\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoWvM32-vHcH"
      },
      "source": [
        "### trash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kxrINkVHrAi"
      },
      "source": [
        "\n",
        "## Hypergraph Neural Network (HGNN) Layer\n",
        "\n",
        "The [HGNN layer](https://arxiv.org/pdf/1809.09401.pdf) is defined as:\n",
        "\n",
        "$$f(X^{(l)}, H; W^{(l)}) = \\sigma(L X^{(l)} W^{(l)})$$$$L = D_v^{-1/2} H B D_e^{-1} H^\\top D_v^{-1/2}$$\n",
        "\n",
        "where\n",
        "\n",
        "* $H \\in \\mathbb{R}^{N \\times M}$ is the incidence matrix of hypergraph with $N$ nodes and $M$ hyperedges.\n",
        "* $D_v \\in \\mathbb{R}^{N \\times N}$ is a diagonal matrix representing node degrees, whose $i$-th diagonal element is $\\sum_{j=1}^M H_{ij}$.\n",
        "* $D_e \\in \\mathbb{R}^{M \\times M}$ is a diagonal matrix representing hyperedge degrees, whose $j$-th diagonal element is $\\sum_{i=1}^N H_{ij}$.\n",
        "* $B \\in \\mathbb{R}^{M \\times M}$ is a diagonal matrix representing the hyperedge weights, whose $j$-th diagonal element is the weight of $j$-th hyperedge.  In our example, $B$ is an identity matrix.\n",
        "\n",
        "The following code builds a two-layer HGNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__2tKqL0eaB0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title test\n",
        "# https://colab.research.google.com/github/dmlc/dgl/blob/master/notebooks/sparse/hgnn.ipynb\n",
        "# https://github.com/dmlc/dgl/blob/master/notebooks/sparse/hgnn.ipynb\n",
        "# https://github.com/dmlc/dgl/blob/master/examples/sparse/hgnn.py\n",
        "import torch\n",
        "\n",
        "cite=torch.Tensor([[0, 1, 2, 2, 2, 2, 3, 4, 5, 5, 5, 5, 6, 7, 7, 8, 8, 9, 9, 10],\n",
        "                    [0, 0, 0, 1, 3, 4, 2, 1, 0, 2, 3, 4, 2, 1, 3, 1, 3, 2, 4, 4]])\n",
        "H = torch.sparse_coo_tensor(indices=cite, values=torch.ones(cite.shape[1]),).coalesce()\n",
        "# uncoalesced tensors, may be duplicate coords in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries\n",
        "# vert _ is in hyperedge _\n",
        "print(H.to_dense()) # cols: hyperedges ; rows: verts\n",
        "\n",
        "# print(H) # indices = [[x1,x2,...], [y1,y2,y3,...]]\n",
        "# print(H.to_sparse_csr()) # crow_indices=[row1 got ? elements, row2... , ... ] , col_indices= col idx # https://stackoverflow.com/questions/52299420/scipy-csr-matrix-understand-indptr\n",
        "# print(H.to_sparse_csc()) # ccol_indices = [start count num elements in col], row_indices = row ind\n",
        "# print(H.to_dense().to_sparse_bsr())\n",
        "# print(H.to_sparse_bsc())\n",
        "\n",
        "csr=H.to_sparse_csr()\n",
        "# csr.crow_indices\n",
        "# csr.col_indices\n",
        "# import numpy as np\n",
        "# ss=np.split(csr.col_indices(), csr.crow_indices())[1:-1]\n",
        "ss=torch.split(csr.col_indices(), tuple(torch.diff(csr.crow_indices())))\n",
        "# ss=torch.split(csr.col_indices(), torch.diff(csr.crow_indices()))\n",
        "print(ss)\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "pp=pad_sequence(ss, batch_first=True, padding_value=-1)\n",
        "print(pp)\n",
        "mask=pp<0\n",
        "print(mask)\n",
        "# node_degrees = H.sum(1)\n",
        "# print(\"Node degrees\", node_degrees)\n",
        "# hyperedge_degrees = H.sum(0)\n",
        "# print(\"Hyperedge degrees\", hyperedge_degrees.values())\n",
        "\n",
        "\n",
        "# vmsg=torch.rand(11,2)\n",
        "# svmsg=torch.stack([torch.sum(vmsg[v.to_dense().to(torch.bool)],0) for v in H.T]) # given e, get all vmsgs then aggregate\n",
        "# # print(svmsg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qVdpGWcncedo"
      },
      "outputs": [],
      "source": [
        "# @title requests data\n",
        "import requests\n",
        "url = 'https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz'\n",
        "# response = requests.get(url)\n",
        "open(\"cora.tgz\", \"wb\").write(response.content)\n",
        "\n",
        "import tarfile # os, sys,\n",
        "tar = tarfile.open('cora.tgz', 'r')\n",
        "tar.extractall('/content')\n",
        "\n",
        "import torch\n",
        "\n",
        "content = open(\"cora/cora.content\", \"r\")\n",
        "# print(content.read(10000))\n",
        "# paper id, bag of words bool, category 0-6 # all str\n",
        "rlst = content.read().split('\\n')[:-1] # bec last row is ''\n",
        "pid = [] # paper id\n",
        "bow = [] # bag of words\n",
        "cls = [] # classes\n",
        "# category: Case_Based, Genetic_Algorithms, Neural_Networks, Probabilistic_Methods, Reinforcement_Learning, Rule_Learning, Theory\n",
        "category = {'Case_Based':0, 'Genetic_Algorithms':1, 'Neural_Networks':2, 'Probabilistic_Methods':3, 'Reinforcement_Learning':4, 'Rule_Learning':5, 'Theory':6} # cora\n",
        "for r in rlst:\n",
        "    rr=r.split('\\t')\n",
        "    pid.append(int(rr[0]))\n",
        "    bow.append(list(map(float, rr[1:-1]))) # must be float\n",
        "    cls.append(category[rr[-1]])\n",
        "pid=torch.tensor(pid)\n",
        "X=torch.tensor(bow)\n",
        "Y=torch.tensor(cls)\n",
        "num_classes=7\n",
        "\n",
        "# https://stellargraph.readthedocs.io/en/v1.0.0rc1/demos/node-classification/gcn/gcn-cora-node-classification-example.html\n",
        "# The Cora dataset consists of 2708 scientific publications\n",
        "# classified into one of seven classes.\n",
        "# The citation network consists of 5429 links\n",
        "# Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary.\n",
        "# The dictionary consists of 1433 unique words\n",
        "\n",
        "cites = open(\"cora/cora.cites\", \"r\") # cite relation\n",
        "clst = cites.read().split('\\n')[:-1] # bec last row is ''\n",
        "cite = [] #\n",
        "for c in clst:\n",
        "    cc=c.split('\\t')\n",
        "    cite.append([int(cc[0]),int(cc[1])])\n",
        "cite=torch.tensor(cite) # [5429]\n",
        "\n",
        "ukeys = torch.unique(pid)\n",
        "uvals = torch.arange(len(ukeys))\n",
        "udict = dict(zip(ukeys.tolist(), uvals.tolist())) # assign new id to each paper\n",
        "pid = pid.apply_(udict.get)\n",
        "cite = cite.apply_(udict.get)\n",
        "\n",
        "num_v = len(pid)\n",
        "H = torch.sparse_coo_tensor(indices=cite.T, values=torch.ones(cite.shape[0]), size=(num_v, num_v)).coalesce() # size=(2708, 2708), nnz=5429, layout=torch.sparse_coo\n",
        "id = torch.sparse.spdiags(torch.ones(H.shape[0]),torch.tensor(0),H.shape)\n",
        "H = (id + H).coalesce() # each vert got its hyperedge, contain all cited and itself, [2708, 2708], incedence matrix, |V| hyperedges\n",
        "\n",
        "\n",
        "train_mask, val_mask, test_mask = torch.zeros(3, num_v, dtype=torch.bool)\n",
        "train_mask[:140], val_mask[140:640], test_mask[-1000:] = True, True, True # cora mask\n",
        "# print(len(train_mask))\n",
        "# print(train_mask)\n",
        "# H, X, Y, num_classes, train_mask, val_mask, test_mask = load_data()\n",
        "\n",
        "# print(train_mask, val_mask, test_mask)\n",
        "# print(sum(train_mask), sum(val_mask), sum(test_mask)) # 140), (500), (1000)\n",
        "# print(sum(test_mask[-1000:]))\n",
        "# print(len(test_mask)) # 2708\n",
        "# print(train_mask[140])\n",
        "# [:140], [140:640], [-1000:]\n",
        "\n",
        "# print(H.shape, X.shape, Y.shape) # [2708, 2708], [2708, 1433], [2708]\n",
        "\n",
        "# @title edge/ incidence list\n",
        "\n",
        "# edic = dict((id, [id]) for id in pid.tolist()) # edge list H(E)={e1,e2,e3}={{A,D},{D,E},{A,B,C}}\n",
        "# idic = dict((id, [id]) for id in pid.tolist()) # incidence list {A:{e1,e3}, B:{e3}, C:{e3}, D:{e1,e2}, E:{e2}}\n",
        "elst = [[id] for id in pid.tolist()] # edge list H(E)={e1,e2,e3}={{A,D},{D,E},{A,B,C}}\n",
        "ilst = [[id] for id in pid.tolist()] # incidence list {A:{e1,e3}, B:{e3}, C:{e3}, D:{e1,e2}, E:{e2}}\n",
        "for a,b in cite.tolist():\n",
        "    elst[a].append(b)\n",
        "    ilst[b].append(a)\n",
        "elst = torch.tensor(elst)\n",
        "ilst = torch.tensor(ilst)\n",
        "# print(elst)\n",
        "# print(ilst)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "d80A5M5UN4YI"
      },
      "outputs": [],
      "source": [
        "# @title gpt HMPNN\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HMPNNLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(HMPNNLayer, self).__init__()\n",
        "        self.fc_v = nn.Linear(input_dim, output_dim)\n",
        "        self.fc_w = nn.Linear(input_dim, output_dim)\n",
        "        self.batch_norm = nn.BatchNorm1d(output_dim)\n",
        "\n",
        "    def forward(self, X_v, W_e, M_v):\n",
        "        # Vertex-to-Hyperedge Message Passing\n",
        "        M_v = F.relu(self.fc_v(M_v))\n",
        "        M_v = F.dropout(M_v, p=0.5, training=self.training)  # Adjust dropout as needed\n",
        "        W_e = W_e * M_v  # Element-wise multiplication with adjacency matrix dropout\n",
        "        W_e = W_e.sum(dim=1)\n",
        "\n",
        "        # Hyperedge-to-Vertex Message Passing\n",
        "        W_e = F.relu(self.fc_w(W_e))\n",
        "        W_e = F.dropout(W_e, p=0.5, training=self.training)\n",
        "        M_e = W_e.unsqueeze(2).repeat(1, 1, X_v.size(1))  # Repeat for all vertices in hyperedge\n",
        "        M_e = M_e * X_v  # Element-wise multiplication\n",
        "        M_e = M_e.sum(dim=1)\n",
        "\n",
        "        # Aggregation and Batch Normalization\n",
        "        M_e = self.batch_norm(M_e)\n",
        "\n",
        "        return M_e\n",
        "\n",
        "class HMPNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(HMPNN, self).__init__()\n",
        "        self.layer1 = HMPNNLayer(input_dim, hidden_dim)\n",
        "        self.layer2 = HMPNNLayer(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, X_v, W_e, M_v):\n",
        "        # Forward pass through layers\n",
        "        M_e = self.layer1(X_v, W_e, M_v)\n",
        "        M_e = self.layer2(X_v, W_e, M_e)\n",
        "\n",
        "        # Final activation\n",
        "        output = self.sigmoid(M_e)\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "input_dim = 64  # Adjust based on your input data\n",
        "hidden_dim = 32\n",
        "# output_dim = 1  # Assuming binary classification\n",
        "model = HMPNN(input_dim, hidden_dim, output_dim)\n",
        "model=HMPNN(num_classes, vembdim, eembdim, vmsgdim, emsgdim)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import BatchNorm1d, Dropout\n",
        "\n",
        "class HMPNNLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout_rate):\n",
        "        super(HMPNNLayer, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "        self.batch_norm = BatchNorm1d(out_features)\n",
        "        self.dropout = Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class HMPNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_rate_v, dropout_rate_e):\n",
        "        super(HMPNN, self).__init__()\n",
        "        self.fv = HMPNNLayer(input_size, hidden_size, dropout_rate_v)\n",
        "        self.fw = HMPNNLayer(hidden_size, output_size, dropout_rate_e)\n",
        "\n",
        "    def forward(self, vemb, H):\n",
        "        vmsg = self.fv(vemb)\n",
        "        aH = F.dropout(H, p=0.7, training=self.training)\n",
        "        eemb = torch.matmul(aH, vmsg)\n",
        "        emsg = self.fw(eemb)\n",
        "        vmsg = torch.matmul(H.T, emsg)\n",
        "        vemb = vemb + vmsg\n",
        "        return F.sigmoid(vemb)\n",
        "\n",
        "# Example Usage:\n",
        "input_size = 64  # Input feature size for vertices\n",
        "hidden_size = 32  # Hidden layer size\n",
        "output_size = 1  # Output size (for binary classification, for example)\n",
        "dropout_rate_v = 0.5  # Dropout rate for vertices\n",
        "dropout_rate_e = 0.5  # Dropout rate for hyperedges\n",
        "\n",
        "# Instantiate the HMPNN model\n",
        "hmpnn_model = HMPNN(input_size, hidden_size, output_size, dropout_rate_v, dropout_rate_e)\n",
        "\n",
        "# Dummy data\n",
        "vemb = torch.randn((batch_size, input_size))\n",
        "hyperedge_adjacency_matrix = torch.randn((batch_size, batch_size))\n",
        "\n",
        "# Forward pass\n",
        "output = hmpnn_model(vemb, hyperedge_adjacency_matrix)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NI94n2pfDX71"
      },
      "outputs": [],
      "source": [
        "# @title pyt-team/TopoModelX data\n",
        "!pip install torch_geometric\n",
        "\n",
        "import torch\n",
        "import torch_geometric.datasets as geom_datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = geom_datasets.Planetoid(root=\"tmp/\", name=\"cora\")[0]\n",
        "\n",
        "incidence_1 = torch.sparse_coo_tensor(dataset[\"edge_index\"], torch.ones(dataset[\"edge_index\"].shape[1]), dtype=torch.long)\n",
        "dataset = dataset.to(device)\n",
        "\n",
        "x_0s = dataset[\"x\"]\n",
        "y = dataset[\"y\"]\n",
        "# print(incidence_1.shape, x_0s.shape, y.shape) # [2708, 2708], [2708, 1433], [2708]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8RnPmhLzAjrY"
      },
      "outputs": [],
      "source": [
        "# @title pyt-team/TopoModelX hmpnn\n",
        "# https://arxiv.org/pdf/2203.16995.pdf\n",
        "# https://github.com/pyt-team/TopoModelX/tree/main/topomodelx/nn/hypergraph\n",
        "# https://github.com/pyt-team/TopoModelX/blob/main/topomodelx/nn/hypergraph/hmpnn.py\n",
        "# https://github.com/pyt-team/TopoModelX/blob/main/tutorials/hypergraph/hmpnn_train.ipynb\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# https://github.com/pyt-team/TopoModelX/blob/main/topomodelx/utils/scatter.py\n",
        "\n",
        "def broadcast(src, other, dim):\n",
        "    \"\"\"Broadcasts `src` to the shape of `other`.\"\"\"\n",
        "    if dim < 0:\n",
        "        dim = other.dim() + dim\n",
        "    if src.dim() == 1:\n",
        "        for _ in range(0, dim):\n",
        "            src = src.unsqueeze(0)\n",
        "    for _ in range(src.dim(), other.dim()):\n",
        "        src = src.unsqueeze(-1)\n",
        "    src = src.expand(other.size())\n",
        "    return src\n",
        "\n",
        "\n",
        "def scatter_sum(src, index, dim = -1, out = None, dim_size = None,):\n",
        "    \"\"\"Add all values from the `src` tensor into `out` at the indices.\"\"\"\n",
        "    index = broadcast(index, src, dim)\n",
        "    if out is None:\n",
        "        size = list(src.size())\n",
        "        if dim_size is not None:\n",
        "            size[dim] = dim_size\n",
        "        elif index.numel() == 0:\n",
        "            size[dim] = 0\n",
        "        else:\n",
        "            size[dim] = int(index.max()) + 1\n",
        "        out = torch.zeros(size, dtype=src.dtype, device=src.device)\n",
        "        return out.scatter_add_(dim, index, src)\n",
        "    else:\n",
        "        return out.scatter_add_(dim, index, src)\n",
        "\n",
        "\n",
        "def scatter_add(src, index, dim = -1, out = None, dim_size = None,):\n",
        "    \"\"\"Add all values from the `src` tensor into `out` at the indices.\"\"\"\n",
        "    return scatter_sum(src, index, dim, out, dim_size)\n",
        "\n",
        "def scatter_mean(src, index, dim = -1, out = None, dim_size = None,):\n",
        "    \"\"\"Compute the mean value of all values from the `src` tensor into `out`.\"\"\"\n",
        "    out = scatter_sum(src, index, dim, out, dim_size)\n",
        "    dim_size = out.size(dim)\n",
        "\n",
        "    index_dim = dim\n",
        "    if index_dim < 0:\n",
        "        index_dim = index_dim + src.dim()\n",
        "    if index.dim() <= index_dim:\n",
        "        index_dim = index.dim() - 1\n",
        "    ones = torch.ones(index.size(), dtype=src.dtype, device=src.device)\n",
        "    count = scatter_sum(ones, index, index_dim, None, dim_size)\n",
        "    count[count < 1] = 1\n",
        "    count = broadcast(count, out, dim)\n",
        "    if out.is_floating_point():\n",
        "        out.true_divide_(count)\n",
        "    else:\n",
        "        out.div_(count, rounding_mode=\"floor\")\n",
        "    return out\n",
        "\n",
        "\n",
        "SCATTER_DICT = {\"sum\": scatter_sum, \"mean\": scatter_mean, \"add\": scatter_sum}\n",
        "\n",
        "\n",
        "def scatter(scatter: str):\n",
        "    if isinstance(scatter, str) and scatter in SCATTER_DICT:\n",
        "        return SCATTER_DICT[scatter]\n",
        "    else:\n",
        "        raise ValueError(f\"scatter must be callable or string: {list(SCATTER_DICT.keys())}\")\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "# https://github.com/pyt-team/TopoModelX/blob/main/topomodelx/base/message_passing.py\n",
        "class MessagePassing(torch.nn.Module):\n",
        "    def __init__(self, aggr_func = \"sum\", att = False, initialization = \"xavier_uniform\", initialization_gain = 1.414,):\n",
        "        # aggr_func: [\"sum\", \"mean\", \"add\"] = \"sum\",\n",
        "        # initialization: [\"uniform\", \"xavier_uniform\", \"xavier_normal\"] = \"xavier_uniform\",\n",
        "        super().__init__()\n",
        "        self.aggr_func = aggr_func\n",
        "        self.att = att\n",
        "        self.initialization = initialization\n",
        "        self.initialization_gain = initialization_gain\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        match self.initialization:\n",
        "            case \"uniform\":\n",
        "                if self.weight is not None:\n",
        "                    stdv = 1.0 / math.sqrt(self.weight.size(1))\n",
        "                    self.weight.data.uniform_(-stdv, stdv)\n",
        "                if self.att:\n",
        "                    stdv = 1.0 / math.sqrt(self.att_weight.size(1))\n",
        "                    self.att_weight.data.uniform_(-stdv, stdv)\n",
        "            case \"xavier_uniform\":\n",
        "                if self.weight is not None: torch.nn.init.xavier_uniform_(self.weight, gain=self.initialization_gain)\n",
        "                if self.att: torch.nn.init.xavier_uniform_(self.att_weight.view(-1, 1), gain=self.initialization_gain)\n",
        "            case \"xavier_normal\":\n",
        "                if self.weight is not None: torch.nn.init.xavier_normal_(self.weight, gain=self.initialization_gain)\n",
        "                if self.att: torch.nn.init.xavier_normal_(self.att_weight.view(-1, 1), gain=self.initialization_gain)\n",
        "            case _: raise ValueError(f\"Initialization {self.initialization} not recognized.\")\n",
        "\n",
        "    def message(self, x_source, x_target=None):\n",
        "        return x_source\n",
        "\n",
        "    def attention(self, x_source, x_target=None):\n",
        "        x_source_per_message = x_source[self.source_index_j]\n",
        "        x_target_per_message = (x_source[self.target_index_i] if x_target is None else x_target[self.target_index_i])\n",
        "        x_source_target_per_message = torch.cat([x_source_per_message, x_target_per_message], dim=1)\n",
        "        return torch.nn.functional.elu(torch.matmul(x_source_target_per_message, self.att_weight))\n",
        "\n",
        "    def aggregate(self, x_message):\n",
        "        aggr = scatter(self.aggr_func)\n",
        "        return aggr(x_message, self.target_index_i, 0)\n",
        "\n",
        "    def forward(self, x_source, neighborhood, x_target=None):\n",
        "        neighborhood = neighborhood.coalesce()\n",
        "        self.target_index_i, self.source_index_j = neighborhood.indices()\n",
        "        neighborhood_values = neighborhood.values()\n",
        "\n",
        "        x_message = self.message(x_source=x_source, x_target=x_target)\n",
        "        x_message = x_message.index_select(-2, self.source_index_j)\n",
        "\n",
        "        if self.att:\n",
        "            attention_values = self.attention(x_source=x_source, x_target=x_target)\n",
        "            neighborhood_values = torch.multiply(neighborhood_values, attention_values)\n",
        "\n",
        "        x_message = neighborhood_values.view(-1, 1) * x_message\n",
        "        return self.aggregate(x_message)\n",
        "\n",
        "\n",
        "\n",
        "class _AdjacencyDropoutMixin:\n",
        "    def apply_dropout(self, neighborhood, dropout_rate):\n",
        "        neighborhood = neighborhood.coalesce()\n",
        "        return torch.sparse_coo_tensor(neighborhood.indices(), F.dropout(neighborhood.values().to(torch.float), dropout_rate), neighborhood.size(),).coalesce()\n",
        "\n",
        "\n",
        "class _NodeToHyperedgeMessenger(MessagePassing, _AdjacencyDropoutMixin):\n",
        "    def __init__(self, messaging_func, adjacency_dropout = 0.7, aggr_func = \"sum\",):\n",
        "        super().__init__(aggr_func)\n",
        "        self.messaging_func = messaging_func\n",
        "        self.adjacency_dropout = adjacency_dropout\n",
        "\n",
        "    def message(self, x_source):\n",
        "        return self.messaging_func(x_source)\n",
        "\n",
        "    def forward(self, x_source, neighborhood):\n",
        "        neighborhood = self.apply_dropout(neighborhood, self.adjacency_dropout)\n",
        "        source_index_j, self.target_index_i = neighborhood.indices()\n",
        "        x_message = self.message(x_source)\n",
        "        x_message_aggregated = self.aggregate(x_message.index_select(-2, source_index_j))\n",
        "        return x_message_aggregated, x_message\n",
        "\n",
        "\n",
        "class _HyperedgeToNodeMessenger(MessagePassing, _AdjacencyDropoutMixin):\n",
        "    def __init__(self, messaging_func, adjacency_dropout = 0.7, aggr_func = \"sum\",):\n",
        "        super().__init__(aggr_func)\n",
        "        self.messaging_func = messaging_func\n",
        "        self.adjacency_dropout = adjacency_dropout\n",
        "\n",
        "    def message(self, x_source, neighborhood, node_messages):\n",
        "        hyperedge_neighborhood = self.apply_dropout(neighborhood, self.adjacency_dropout)\n",
        "        source_index_j, target_index_i = hyperedge_neighborhood.indices()\n",
        "        node_messages_aggregated = scatter(self.aggr_func)(node_messages.index_select(-2, source_index_j), target_index_i, 0)\n",
        "        return self.messaging_func(x_source, node_messages_aggregated)\n",
        "\n",
        "    def forward(self, x_source, neighborhood, node_messages):\n",
        "        x_message = self.message(x_source, neighborhood, node_messages)\n",
        "        neighborhood = self.apply_dropout(neighborhood, self.adjacency_dropout)\n",
        "        self.target_index_i, source_index_j = neighborhood.indices()\n",
        "        x_message_aggregated = self.aggregate(x_message.index_select(-2, source_index_j))\n",
        "        return x_message_aggregated\n",
        "\n",
        "class _DefaultHyperedgeToNodeMessagingFunc(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__() self.linear = nn.Linear(2 * in_channels, in_channels)\n",
        "    def forward(self, x_1, m_0): return F.sigmoid(self.linear(torch.cat((x_1, m_0), dim=1)))\n",
        "\n",
        "class _DefaultUpdatingFunc(nn.Module):\n",
        "    def __init__(self, in_channels): super().__init__()\n",
        "    def forward(self, x, m): return F.sigmoid(x + m)\n",
        "\n",
        "\n",
        "class HMPNNLayer(nn.Module):\n",
        "    def __init__(self, in_channels, node_to_hyperedge_messaging_func=None, hyperedge_to_node_messaging_func=None, adjacency_dropout = 0.7, aggr_func = \"sum\", updating_dropout = 0.5, updating_func=None,):\n",
        "        super().__init__()\n",
        "        if node_to_hyperedge_messaging_func is None:\n",
        "            node_to_hyperedge_messaging_func = nn.Sequential(nn.Linear(in_channels, in_channels), nn.Sigmoid())\n",
        "        self.node_to_hyperedge_messenger = _NodeToHyperedgeMessenger(node_to_hyperedge_messaging_func, adjacency_dropout, aggr_func)\n",
        "        if hyperedge_to_node_messaging_func is None:\n",
        "            hyperedge_to_node_messaging_func = _DefaultHyperedgeToNodeMessagingFunc(in_channels)\n",
        "        self.hyperedge_to_node_messenger = _HyperedgeToNodeMessenger(hyperedge_to_node_messaging_func, adjacency_dropout, aggr_func)\n",
        "        self.node_batchnorm = nn.BatchNorm1d(in_channels)\n",
        "        self.hyperedge_batchnorm = nn.BatchNorm1d(in_channels)\n",
        "        self.dropout = torch.distributions.Bernoulli(updating_dropout)\n",
        "\n",
        "        if updating_func is None:\n",
        "            updating_func = _DefaultUpdatingFunc(in_channels)\n",
        "        self.updating_func = updating_func\n",
        "\n",
        "    def apply_regular_dropout(self, x):\n",
        "        if self.training:\n",
        "            mask = self.dropout.sample(x.shape).to(dtype=torch.float, device=x.device)\n",
        "            d = x.size(0)\n",
        "            x *= mask * (2 * d - mask.sum(dim=1)).view(-1, 1) / d\n",
        "        return x\n",
        "\n",
        "    def forward(self, x_0, x_1, incidence_1):\n",
        "        node_messages_aggregated, node_messages = self.node_to_hyperedge_messenger( x_0, incidence_1)\n",
        "        hyperedge_messages_aggregated = self.hyperedge_to_node_messenger(x_1, incidence_1, node_messages)\n",
        "        x_0 = self.updating_func(self.apply_regular_dropout(self.node_batchnorm(x_0)), hyperedge_messages_aggregated,)\n",
        "        x_1 = self.updating_func(self.apply_regular_dropout(self.hyperedge_batchnorm(x_1)), node_messages_aggregated,)\n",
        "        return x_0, x_1\n",
        "\n",
        "\n",
        "class HMPNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, n_layers=2, adjacency_dropout_rate=0.7, regular_dropout_rate=0.5,):\n",
        "        super().__init__()\n",
        "        self.linear_node = torch.nn.Linear(in_channels, hidden_channels)\n",
        "        self.linear_edge = torch.nn.Linear(in_channels, hidden_channels)\n",
        "        self.layers = torch.nn.ModuleList([HMPNNLayer(hidden_channels, adjacency_dropout=adjacency_dropout_rate, updating_dropout=regular_dropout_rate,) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x_0, x_1, incidence_1):\n",
        "        x_0 = self.linear_node(x_0)\n",
        "        x_1 = self.linear_edge(x_1)\n",
        "        for layer in self.layers:\n",
        "            x_0, x_1 = layer(x_0, x_1, incidence_1)\n",
        "        return x_0, x_1\n",
        "\n",
        "\n",
        "class Network(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, task_level=\"graph\", **kwargs): # task_level: \"graph\" or \"node\".\n",
        "        super().__init__()\n",
        "        self.base_model = HMPNN(in_channels=in_channels, hidden_channels=hidden_channels, **kwargs)\n",
        "        self.linear = torch.nn.Linear(hidden_channels, out_channels)\n",
        "        self.out_pool = True if task_level == \"graph\" else False\n",
        "\n",
        "    # def forward(self, x_0, x_1, incidence_1):\n",
        "    def forward(self, incidence_1, x_0):\n",
        "        x_1 = torch.zeros_like(x_0)\n",
        "\n",
        "        x_0, x_1 = self.base_model(x_0, x_1, incidence_1)\n",
        "        if self.out_pool is True: x = torch.max(x_0, dim=0)[0]\n",
        "        else: x = x_0\n",
        "        return self.linear(x)\n",
        "\n",
        "# Base model hyperparameters\n",
        "in_channels = x_0s.shape[1]\n",
        "hidden_channels = 128\n",
        "n_layers = 1\n",
        "\n",
        "# Readout hyperparameters\n",
        "out_channels = torch.unique(y).shape[0]\n",
        "task_level = \"graph\" if out_channels == 1 else \"node\"\n",
        "\n",
        "# model = Network(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=out_channels, n_layers=n_layers, task_level=task_level,).to(device)\n",
        "# print(in_channels, hidden_channels, out_channels, n_layers, task_level) # 1433, 128, 7, 1, node\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IqJZNt-gESzv"
      },
      "outputs": [],
      "source": [
        "# @title pyt-team/TopoModelX run\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "train_mask = dataset[\"train_mask\"]\n",
        "val_mask = dataset[\"val_mask\"]\n",
        "test_mask = dataset[\"test_mask\"]\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "initial_x_1 = torch.zeros_like(x_0s)\n",
        "for epoch in range(2):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    # print(x_0s.shape, initial_x_1.shape, incidence_1.shape)\n",
        "    # print(x_0s, initial_x_1, incidence_1) # 0s? [2708, 1433], 0s [2708, 1433], sparse coo [2708, 2708]\n",
        "    # y_hat = model(x_0s, initial_x_1, incidence_1)\n",
        "    y_hat = model(incidence_1, x_0s)\n",
        "    loss = loss_fn(y_hat[train_mask], y[train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss = loss.item()\n",
        "    y_pred = y_hat.argmax(dim=-1)\n",
        "    train_acc = accuracy_score(y[train_mask].cpu(), y_pred[train_mask].cpu())\n",
        "    # print((y[train_mask]==y_pred[train_mask]).sum()/len(y[train_mask]))\n",
        "    # train_acc = accuracy(y[train_mask], y_pred[train_mask])\n",
        "\n",
        "    model.eval()\n",
        "    # y_hat = model(x_0s, initial_x_1, incidence_1)\n",
        "    y_hat = model(incidence_1, x_0s)\n",
        "    val_loss = loss_fn(y_hat[val_mask], y[val_mask]).item()\n",
        "    y_pred = y_hat.argmax(dim=-1)\n",
        "    # val_acc = accuracy_score(y[val_mask].cpu(), y_pred[val_mask].cpu())\n",
        "\n",
        "    test_loss = loss_fn(y_hat[test_mask], y[test_mask]).item()\n",
        "    y_pred = y_hat.argmax(dim=-1)\n",
        "    test_acc = accuracy_score(y[test_mask].cpu(), y_pred[test_mask].cpu())\n",
        "    # test_acc = accuracy(y[test_mask], y_pred[test_mask])\n",
        "    print(f\"{epoch + 1} train loss: {train_loss:.4f} test loss: {test_loss:.4f} test acc: {test_acc:.2f}\") # val loss: {val_loss:.4f} val acc: {val_acc:.2f}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEgVIiRUKQI8",
        "outputId": "bd9d4015-6e8f-4863-b50e-2a2dd61484bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2708, 7])\n"
          ]
        }
      ],
      "source": [
        "# @title HMPNN me H\n",
        "# https://arxiv.org/pdf/2203.16995.pdf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# Vert msg = fv(vert ebd) , Sum edge msgs\n",
        "# Edge msg = fw(edge emb, Sum Vert msgs)\n",
        "# Vert emb1 = gv(vert emb, Sum edge msgs)\n",
        "# Edge emb1 = gw(edge emb, Sum Vert msgs)\n",
        "\n",
        "class MsgPass(nn.Module):\n",
        "    def __init__(self, vembdim, eembdim, vmsgdim, emsgdim):\n",
        "    def __init__(self, in_dim, hid_dim, out_dim):\n",
        "        super(MsgPass, self).__init__()\n",
        "        self.h_dim = 16\n",
        "        self.fv = nn.Sequential(\n",
        "            # nn.Linear(vembdim, self.h_dim), nn.ReLU(),\n",
        "            # nn.Linear(vembdim, self.h_dim), nn.Sigmoid(),\n",
        "            # nn.Linear(self.h_dim, vmsgdim),\n",
        "            # nn.Linear(vembdim, vmsgdim), nn.ReLU(),\n",
        "            nn.Linear(vembdim, vmsgdim), #nn.Sigmoid(),\n",
        "            )\n",
        "        self.fw = nn.Sequential(\n",
        "            # nn.Linear(eembdim+vmsgdim, self.h_dim), nn.ReLU(),\n",
        "            # nn.Linear(eembdim+vmsgdim, self.h_dim), nn.Sigmoid(),\n",
        "            # nn.Linear(self.h_dim, emsgdim),\n",
        "            # nn.Linear(eembdim+vmsgdim, emsgdim), nn.ReLU(),\n",
        "            nn.Linear(eembdim+vmsgdim, emsgdim), #nn.Sigmoid(),\n",
        "            )\n",
        "        self.gv = nn.Sequential(\n",
        "            # nn.Linear(vembdim+emsgdim, self.h_dim), nn.ReLU(),\n",
        "            # nn.Linear(vembdim+emsgdim, self.h_dim), nn.Sigmoid(),\n",
        "            # nn.Linear(self.h_dim, vembdim),\n",
        "            # nn.Linear(vembdim+emsgdim, vembdim), nn.ReLU(),\n",
        "            nn.Linear(vembdim+emsgdim, vembdim), #nn.Sigmoid(),\n",
        "            )\n",
        "        self.gw = nn.Sequential(\n",
        "            # nn.Linear(eembdim+vmsgdim, self.h_dim), nn.ReLU(),\n",
        "            # nn.Linear(eembdim+vmsgdim, self.h_dim), nn.Sigmoid(),\n",
        "            # nn.Linear(self.h_dim, eembdim),\n",
        "            # nn.Linear(eembdim+vmsgdim, eembdim), nn.ReLU(),\n",
        "            nn.Linear(eembdim+vmsgdim, eembdim), #nn.Sigmoid(),\n",
        "            )\n",
        "        # self.vmsgdim = vmsgdim\n",
        "        # self.emsgdim = emsgdim\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "        self.adjdrop = AdjDropout(0.7)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, H, vemb, eemb, emsg=None):\n",
        "        vemb, eemb = self.drop(vemb), self.drop(eemb)\n",
        "        vmsg = self.fv(vemb)\n",
        "        # vmsg = self.drop(vmsg)\n",
        "        # print(\"vemb, eemb\",vemb.shape, eemb.shape) # [2708, 2], [2708, 2]\n",
        "        H = self.adjdrop(H)\n",
        "        HT = H.T\n",
        "        vmsg = self.sig(vmsg)\n",
        "        svmsg = HT @ vmsg # sum aggregate\n",
        "        # svmsg = vmsg @ H\n",
        "        # svmsg = self.sig(svmsg)\n",
        "        # print(\"vmsg, svmsg\",vmsg.shape, svmsg.shape) # [2708, 2], [2708, 2]\n",
        "        emsg = self.fw(torch.cat((eemb, svmsg), 1))\n",
        "        # emsg = self.drop(emsg)\n",
        "        emsg = self.sig(emsg)\n",
        "        semsg = H @ emsg\n",
        "        # semsg = self.sig(semsg)\n",
        "        vemb1 = self.gv(torch.cat((vemb, semsg), 1))\n",
        "        # vemb1 = self.drop(vemb1)\n",
        "        vmsg = self.sig(vmsg)\n",
        "        svmsg = HT @ vmsg\n",
        "        # svmsg = self.sig(svmsg)\n",
        "        eemb1 = self.gw(torch.cat((eemb, svmsg), 1))\n",
        "        # eemb1 = self.drop(eemb1)\n",
        "\n",
        "\n",
        "        # vmsg = self.fv(vemb)\n",
        "        # vmsg = D_v_invsqrt @ vmsg # outgoing node msg is node features mul by inv sqrt of their deg, i.e. Dv^-1/2 X(l)\n",
        "        # svmsg = self.adjdrop(H).T @ vmsg # sum aggregate\n",
        "        # # svmsg = self.adjdrop(H).T @ D_v_invsqrt @ vmsg # node aggregation function is sum of input multiplied by the inverse square root of their degree, i.e. Dv^-1/2 H\n",
        "        # # emsg = self.fw(torch.cat((eemb, svmsg), 1))\n",
        "        # emsg=eemb\n",
        "        # semsg = self.adjdrop(H) @ emsg\n",
        "        # semsg = D_e_inv @ semsg # hyperedge aggregation is the average, i.e. De^-1 HT\n",
        "        # vemb1 = self.gv(torch.cat((vemb, semsg), 1)) # node updating function is σ(XΘ(l))\n",
        "        # vemb1 = D_v_invsqrt @ vemb1 #\n",
        "        # svmsg = self.adjdrop(H).T @ vmsg\n",
        "        # eemb1 = self.gw(torch.cat((eemb, svmsg), 1))\n",
        "\n",
        "        return vemb1, eemb1, emsg\n",
        "\n",
        "class AdjDropout(nn.Module):\n",
        "    def __init__(self, p=0.7):\n",
        "        super(AdjDropout, self).__init__()\n",
        "    def forward(self, H):\n",
        "        mask = (torch.rand(n_e) >= p).float().expand(n_v,n_e) # 1->keep, throw p\n",
        "        return H*mask\n",
        "\n",
        "class HMPNN(nn.Module):\n",
        "    def __init__(self, outdim, vembdim, eembdim, vmsgdim, emsgdim):\n",
        "        super(HMPNN, self).__init__()\n",
        "        # self.msgpass = MsgPass(vembdim, eembdim, vmsgdim, emsgdim)\n",
        "        self.msgpass = MsgPass(X.size(1), eembdim, vmsgdim, emsgdim)\n",
        "        self.msgpass2 = MsgPass(vembdim, eembdim, vmsgdim, emsgdim)\n",
        "        self.msgpass3 = MsgPass(vembdim, eembdim, vmsgdim, emsgdim)\n",
        "        self.lin = nn.Linear(vembdim, outdim)\n",
        "        # vert 1/0 emb provided\n",
        "        # self.ve = nn.Embedding(vdim, vembdim) # turn vects from 1/0 vect to vect emb\n",
        "        # self.ee = nn.Embedding(edim, eembdim)\n",
        "        self.eemb = None\n",
        "        # create edge vect emb\n",
        "        self.eembdim = eembdim\n",
        "        # self.ve = nn.Linear(X.size(-1), vembdim, bias=False)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        # vemb = self.ve(X)\n",
        "        # print(\"vemb\",vemb.shape)\n",
        "        eemb = torch.zeros(len(elst),self.eembdim)\n",
        "        # eemb = self.ee(eemb)\n",
        "        vemb1, eemb1, emsg = self.msgpass(H, vemb, eemb)\n",
        "        vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        vemb1, eemb1, emsg = self.msgpass2(H, vemb, eemb, emsg=emsg)\n",
        "        vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        # vemb1, eemb1, emsg = self.msgpass3(H, vemb, eemb, emsg=emsg)\n",
        "        # vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        x = self.lin(vemb)\n",
        "        return x\n",
        "\n",
        "num_v,vdim=X.shape\n",
        "# print(\"num_v,vembdim\",num_v,vembdim) # 2708, 1433\n",
        "# vembdim, eembdim = 2, 2\n",
        "# vmsgdim, emsgdim = 2, 2\n",
        "vembdim=eembdim=vmsgdim=emsgdim=16\n",
        "\n",
        "num_classes=7\n",
        "model=HMPNN(num_classes, vembdim, eembdim, vmsgdim, emsgdim)\n",
        "# yhat = model(X, elst, ilst)\n",
        "# print(H.shape)\n",
        "yhat = model(H, X)\n",
        "print(yhat.shape) # [2708, 7]\n",
        "\n",
        "# Implementation Details Our model uses two layers of HMPNN with sigmoid\n",
        "# activation and a hidden representation of size 2. We use sum as the message\n",
        "# aggregation functions, with adjacency matrix dropout with rate 0.7, as well as\n",
        "# dropout with rate 0.5 for vertex and hyperedge representation.\n",
        "\n",
        "# print(len(X[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pPCFxBcvFsH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title HMPNN elst, ilst\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# Vert msg = fv(vert ebd)\n",
        "# Edge msg = fw(edge emb, Sum Vert msgs)\n",
        "# Vert emb1 = gv(vert emb, Sum edge msgs)\n",
        "# Edge emb1 = gw(edge emb, Sum Vert msgs)\n",
        "\n",
        "class MsgPass(nn.Module):\n",
        "    def __init__(self, vembdim, eembdim, vmsgdim, emsgdim):\n",
        "        super(MsgPass, self).__init__()\n",
        "        self.h_dim = 4\n",
        "        self.fv = nn.Sequential(\n",
        "            # nn.Linear(vembdim, self.h_dim), nn.ReLU(),\n",
        "            # nn.Linear(vembdim, self.h_dim), nn.Sigmoid(),\n",
        "            # nn.Linear(self.h_dim, vmsgdim),\n",
        "            # nn.Linear(vembdim, vmsgdim), nn.ReLU(),\n",
        "            nn.Linear(vembdim, vmsgdim), nn.Sigmoid(),\n",
        "            )\n",
        "        self.fw = nn.Sequential(\n",
        "            # nn.Linear(eembdim+vmsgdim, self.h_dim), nn.ReLU(),\n",
        "            # nn.Linear(eembdim+vmsgdim, self.h_dim), nn.Sigmoid(),\n",
        "            # nn.Linear(self.h_dim, emsgdim),\n",
        "            # nn.Linear(eembdim+vmsgdim, emsgdim), nn.ReLU(),\n",
        "            nn.Linear(eembdim+vmsgdim, emsgdim), nn.Sigmoid(),\n",
        "            )\n",
        "        self.gv = nn.Sequential(\n",
        "            # nn.Linear(vembdim+emsgdim, self.h_dim), nn.ReLU(),\n",
        "            # nn.Linear(vembdim+emsgdim, self.h_dim), nn.Sigmoid(),\n",
        "            # nn.Linear(self.h_dim, vembdim),\n",
        "            # nn.Linear(vembdim+emsgdim, vembdim), nn.ReLU(),\n",
        "            nn.Linear(vembdim+emsgdim, vembdim), nn.Sigmoid(),\n",
        "            )\n",
        "        self.gw = nn.Sequential(\n",
        "            # nn.Linear(eembdim+vmsgdim, self.h_dim), nn.ReLU(),\n",
        "            # nn.Linear(eembdim+vmsgdim, self.h_dim), nn.Sigmoid(),\n",
        "            # nn.Linear(self.h_dim, eembdim),\n",
        "            # nn.Linear(eembdim+vmsgdim, eembdim), nn.ReLU(),\n",
        "            nn.Linear(eembdim+vmsgdim, eembdim), nn.Sigmoid(),\n",
        "            )\n",
        "        self.vmsgdim = vmsgdim\n",
        "        self.emsgdim = emsgdim\n",
        "\n",
        "    def forward(self, vemb, eemb, elst, ilst):\n",
        "        # semsg=torch.stack([torch.mean(emsg[e],0) for e in ilst]) # given v, get all emsgs then aggregate\n",
        "        vmsg = self.fv(vemb)\n",
        "        # vmsg = self.fv(torch.cat((vemb, semsg), 1))\n",
        "        # vmsg = F.dropout(F.batch_norm(vmsg,torch.zeros(self.vmsgdim),torch.ones(self.vmsgdim)),p=0.5)\n",
        "\n",
        "        mvemb, meemb, melst, milst = vemb, eemb, elst, ilst\n",
        "        # _, meemb, melst, milst = adjdrop(vemb, eemb, elst, ilst, p=0.7)\n",
        "\n",
        "        svmsg=torch.stack([torch.sum(vmsg[v],0) for v in melst]) # given e, get all vmsgs then aggregate\n",
        "        memsg = self.fw(torch.cat((meemb, svmsg), 1))\n",
        "        # memsg = F.dropout(F.batch_norm(memsg,torch.zeros(self.emsgdim),torch.ones(self.emsgdim)),p=0.5)\n",
        "\n",
        "        semsg=torch.stack([torch.sum(memsg[e],0) for e in milst]) # given v, get all emsgs then aggregate # cannot be mean bec vert in ilst may be isolated, divide by 0 hyperedges\n",
        "        vemb1 = self.gv(torch.cat((vemb, semsg), 1))\n",
        "\n",
        "        svmsg=torch.stack([torch.sum(vmsg[v],0) for v in elst]) # given e, get all vmsgs then aggregate\n",
        "        eemb1 = self.gw(torch.cat((eemb, svmsg), 1))\n",
        "        return vemb1, eemb1\n",
        "\n",
        "# Vert msg = fv(vert ebd)\n",
        "# Edge msg = fw(edge emb, Sum Vert msgs)\n",
        "# Vert emb1 = gv(vert emb, Sum edge msgs)\n",
        "# Edge emb1 = gw(edge emb, Sum Vert msgs)\n",
        "\n",
        "    # def forward(self, vemb, eemb, elst, ilst):\n",
        "    #     vmsg = self.fv(vemb)\n",
        "    #     return vmsg\n",
        "\n",
        "    # def forward(self, vemb, eemb, elst, ilst):\n",
        "    #     svmsg=torch.stack([torch.sum(vmsg[v],0) for v in elst]) # given e, get all vmsgs then aggregate\n",
        "    #     eemb1 = self.gw(torch.cat((eemb, svmsg), 1))\n",
        "\n",
        "    #     svmsg=torch.stack([torch.sum(vmsg[v],0) for v in elst]) # given e, get all vmsgs then aggregate\n",
        "    #     emsg = self.fw(torch.cat((meemb, svmsg), 1))\n",
        "\n",
        "    #     semsg=torch.stack([torch.sum(emsg[e],0) for e in ilst]) # given v, get all emsgs then aggregate # cannot be mean bec vert in ilst may be isolated, divide by 0 hyperedges\n",
        "    #     vemb1 = self.gv(torch.cat((vemb, semsg), 1))\n",
        "\n",
        "    #     return vemb1, eemb1\n",
        "\n",
        "def ilst_from_elst(elst, n_v=len(ilst)): # generate incidence list from edge list\n",
        "    ilst = [[] for id in range(n_v)]\n",
        "    for e,vs in enumerate(elst):\n",
        "        [ilst[v].append(e) for v in vs]\n",
        "    return ilst\n",
        "\n",
        "def adjdrop(vemb, eemb, elst, ilst, p=0.7): # adjacency dropout, maybe can replace with slicing of sparse tensors if pytorch implements it\n",
        "    mask = torch.rand(len(elst)) >= p # True->keep, throw p\n",
        "    melst = [e for e, m in zip(elst, mask) if m]\n",
        "    meemb = eemb[mask==True]\n",
        "    milst = ilst_from_elst(melst)\n",
        "    return vemb, meemb, melst, milst\n",
        "\n",
        "\n",
        "class HMPNN(nn.Module):\n",
        "    def __init__(self, outdim, vembdim, eembdim):\n",
        "        super(HMPNN, self).__init__()\n",
        "        self.msgpass = MsgPass(vembdim, eembdim, vmsgdim=2, emsgdim=2)\n",
        "        self.lin = nn.Linear(vembdim, outdim)\n",
        "\n",
        "    def forward(self, x, elst=elst, ilst=ilst):\n",
        "        vemb = x\n",
        "        eemb = torch.zeros(len(elst),eembdim)\n",
        "        # vemb, eemb = self.msgpass(vemb, eemb)\n",
        "        # vemb, eemb = self.msgpass(vemb, eemb, elst=elst, ilst=ilst)\n",
        "        vemb1, eemb1 = self.msgpass(vemb, eemb, elst=elst, ilst=ilst)\n",
        "        vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        vemb1, eemb1 = self.msgpass(vemb, eemb, elst=elst, ilst=ilst)\n",
        "        vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        vemb1, eemb1 = self.msgpass(vemb, eemb, elst=elst, ilst=ilst)\n",
        "        vemb, eemb = vemb+vemb1, eemb+eemb1\n",
        "        x = self.lin(vemb)\n",
        "        return x\n",
        "\n",
        "\n",
        "def trainl(model, optimizer, elst, ilst, X, Y, train_mask):\n",
        "    model.train()\n",
        "    Y_hat = model(X,elst, ilst)\n",
        "    l,r=0,4\n",
        "    # print(Y_hat[train_mask][l:r], Y[train_mask][l:r])\n",
        "    loss = F.cross_entropy(Y_hat[train_mask], Y[train_mask]) # loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluatel(model, elst, ilst, X, Y, val_mask, test_mask):\n",
        "    model.eval()\n",
        "    Y_hat = model(X,elst, ilst) # model(X)\n",
        "    val_acc = accuracy(Y_hat[val_mask].argmax(1), Y[val_mask])\n",
        "    test_acc = accuracy(Y_hat[test_mask].argmax(1), Y[test_mask])\n",
        "    return val_acc, test_acc\n",
        "\n",
        "# val_acc, test_acc = evaluatel(model, elst, ilst, X, Y, val_mask, test_mask)\n",
        "# val_acc, test_acc = evaluatel(model, elst, ilst, X, Y, val_mask, train_mask)\n",
        "# print(val_acc, test_acc)\n",
        "\n",
        "\n",
        "\n",
        "num_v,vembdim=X.shape\n",
        "eembdim=2\n",
        "num_classes=7\n",
        "model=HMPNN(num_classes, vembdim, eembdim)\n",
        "yhat = model(X, elst, ilst)\n",
        "# print(yhat.shape) # [2708, 7]\n",
        "\n",
        "# print(len(X[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "85UISW2vj7LP"
      },
      "outputs": [],
      "source": [
        "# @title HMPNN H\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# Vert msg = fv(vert ebd)\n",
        "# Edge msg = fw(edge emb, Sum Vert msgs)\n",
        "# Vert emb1 = gv(vert emb, Sum edge msgs)\n",
        "# Edge emb1 = gw(edge emb, Sum Vert msgs)\n",
        "\n",
        "class MsgPass(nn.Module):\n",
        "    def __init__(self, vembdim, eembdim, vmsgdim, emsgdim):\n",
        "        super(MsgPass, self).__init__()\n",
        "        self.h_dim = 4\n",
        "        self.fv = nn.Sequential(\n",
        "            # nn.Linear(vembdim, self.h_dim), nn.ReLU(),\n",
        "            nn.Linear(vembdim, self.h_dim), nn.Sigmoid(),\n",
        "            nn.Linear(self.h_dim, vmsgdim),\n",
        "            # nn.Linear(vembdim, vmsgdim), nn.ReLU(),\n",
        "            # nn.Linear(vembdim, vmsgdim), nn.Sigmoid(),\n",
        "            )\n",
        "        self.fw = nn.Sequential(\n",
        "            # nn.Linear(eembdim+vmsgdim, self.h_dim), nn.ReLU(),\n",
        "            nn.Linear(eembdim+vmsgdim, self.h_dim), nn.Sigmoid(),\n",
        "            nn.Linear(self.h_dim, emsgdim),\n",
        "            # nn.Linear(eembdim+vmsgdim, emsgdim), nn.ReLU(),\n",
        "            # nn.Linear(eembdim+vmsgdim, emsgdim), nn.Sigmoid(),\n",
        "            )\n",
        "        self.gv = nn.Sequential(\n",
        "            # nn.Linear(vembdim+emsgdim, self.h_dim), nn.ReLU(),\n",
        "            nn.Linear(vembdim+emsgdim, self.h_dim), nn.Sigmoid(),\n",
        "            nn.Linear(self.h_dim, vembdim),\n",
        "            # nn.Linear(vembdim+emsgdim, vembdim), nn.ReLU(),\n",
        "            # nn.Linear(vembdim+emsgdim, vembdim), nn.Sigmoid(),\n",
        "            )\n",
        "        self.gw = nn.Sequential(\n",
        "            # nn.Linear(eembdim+vmsgdim, self.h_dim), nn.ReLU(),\n",
        "            nn.Linear(eembdim+vmsgdim, self.h_dim), nn.Sigmoid(),\n",
        "            nn.Linear(self.h_dim, eembdim),\n",
        "            # nn.Linear(eembdim+vmsgdim, eembdim), nn.ReLU(),\n",
        "            # nn.Linear(eembdim+vmsgdim, eembdim), nn.Sigmoid(),\n",
        "            )\n",
        "        self.vmsgdim = vmsgdim\n",
        "        self.emsgdim = emsgdim\n",
        "\n",
        "    def forward(self, H, vemb, eemb):\n",
        "        # semsg=torch.stack([torch.mean(emsg[e],0) for e in ilst]) # given v, get all emsgs then aggregate\n",
        "        vmsg = self.fv(vemb)\n",
        "        # vmsg = self.fv(torch.cat((vemb, semsg), 1))\n",
        "        # vmsg = F.dropout(F.batch_norm(vmsg,torch.zeros(self.vmsgdim),torch.ones(self.vmsgdim)),p=0.5)\n",
        "\n",
        "        # mvemb, meemb, melst, milst = vemb, eemb, elst, ilst\n",
        "        # _, meemb, melst, milst = adjdrop(vemb, eemb, elst, ilst, p=0.7)\n",
        "\n",
        "\n",
        "        # svmsg=torch.stack([torch.sum(vmsg[v],0) for v in melst]) # given e, get all vmsgs then aggregate\n",
        "        svmsg=torch.stack([torch.sum(vmsg[v.to_dense().to(torch.bool)],0) for v in H.T]) # given e, get all vmsgs then aggregate\n",
        "        memsg = self.fw(torch.cat((meemb, svmsg), 1))\n",
        "        memsg = F.dropout(F.batch_norm(memsg,torch.zeros(self.emsgdim),torch.ones(self.emsgdim)),p=0.5)\n",
        "\n",
        "        # cannot be mean bec vert in ilst may be isolated, divide by 0 hyperedges\n",
        "        # semsg=torch.stack([torch.sum(memsg[e],0) for e in milst]) # given v, get all emsgs then aggregate\n",
        "        semsg=torch.stack([torch.sum(memsg[e.to_dense().to(torch.bool)],0) for e in H]) # given e, get all vmsgs then aggregate\n",
        "        vemb1 = self.gv(torch.cat((vemb, semsg), 1))\n",
        "\n",
        "        # svmsg=torch.stack([torch.sum(vmsg[v],0) for v in elst]) # given e, get all vmsgs then aggregate\n",
        "        svmsg=torch.stack([torch.sum(vmsg[v.to_dense().to(torch.bool)],0) for v in H.T]) # given e, get all vmsgs then aggregate\n",
        "        eemb1 = self.gw(torch.cat((eemb, svmsg), 1))\n",
        "        return vemb1, eemb1\n",
        "\n",
        "# def ilst_from_elst(elst, n_v=len(ilst)): # generate incidence list from edge list\n",
        "#     ilst = [[] for id in range(n_v)]\n",
        "#     for e,vs in enumerate(elst):\n",
        "#         [ilst[v].append(e) for v in vs]\n",
        "#     return ilst\n",
        "\n",
        "def adjdrop(vemb, eemb, elst, ilst, p=0.7): # adjacency dropout, maybe can replace with slicing of sparse tensors if pytorch implements it\n",
        "    mask = torch.rand(len(elst)) >= p # True->keep, throw p\n",
        "    melst = [e for e, m in zip(elst, mask) if m]\n",
        "    meemb = eemb[mask==True]\n",
        "    milst = ilst_from_elst(melst)\n",
        "    return vemb, meemb, melst, milst\n",
        "\n",
        "\n",
        "class HMPNN(nn.Module):\n",
        "    def __init__(self, outdim, vembdim, eembdim):\n",
        "        super(HMPNN, self).__init__()\n",
        "        self.msgpass = MsgPass(vembdim, eembdim, vmsgdim=2, emsgdim=2)\n",
        "        self.lin = nn.Linear(vembdim, outdim)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        vemb = x\n",
        "        eemb = torch.zeros(len(elst),eembdim)\n",
        "        # vemb, eemb = self.msgpass(vemb, eemb)\n",
        "        vemb, eemb = self.msgpass(vemb, eemb, elst=elst, ilst=ilst)\n",
        "        vemb, eemb = self.msgpass(vemb, eemb, elst=elst, ilst=ilst)\n",
        "        x = self.lin(vemb)\n",
        "        return x\n",
        "\n",
        "num_v,vembdim=X.shape\n",
        "eembdim=2\n",
        "num_classes=7\n",
        "model=HMPNN(num_classes, vembdim, eembdim)\n",
        "# yhat = model(X, elst, ilst)\n",
        "Y_hat = model(H, X)\n",
        "# print(yhat.shape) # [2708, 7]\n",
        "\n",
        "# print(len(X[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sTRlHcQtEFXF"
      },
      "outputs": [],
      "source": [
        "# @title old og hg attn\n",
        "Hypergraph Convolution and Hypergraph Attention\n",
        "(https://arxiv.org/pdf/1901.08150.pdf).\n",
        "import argparse\n",
        "import dgl.sparse as dglsp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "from dgl.data import CoraGraphDataset\n",
        "def accuracy(yhat, y): return (yhat.argmax(1) == y).type(torch.float).sum().item()/y.shape[0]\n",
        "\n",
        "\n",
        "def hypergraph_laplacian(H):\n",
        "    ###########################################################\n",
        "    # (HIGHLIGHT) Compute the Laplacian with Sparse Matrix API\n",
        "    ###########################################################\n",
        "    d_V = H.sum(1)  # node degree\n",
        "    d_E = H.sum(0)  # edge degree\n",
        "    n_edges = d_E.shape[0]\n",
        "    D_V_invsqrt = dglsp.diag(d_V**-0.5)  # D_V ** (-1/2)\n",
        "    D_E_inv = dglsp.diag(d_E**-1)  # D_E ** (-1)\n",
        "    W = dglsp.identity((n_edges, n_edges))\n",
        "    return D_V_invsqrt @ H @ W @ D_E_inv @ H.T @ D_V_invsqrt\n",
        "\n",
        "\n",
        "class HypergraphAttention(nn.Module):\n",
        "    \"\"\"Hypergraph Attention module as in the paper\n",
        "    `Hypergraph Convolution and Hypergraph Attention\n",
        "    <https://arxiv.org/pdf/1901.08150.pdf>`_.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.P = nn.Linear(in_size, out_size)\n",
        "        self.a = nn.Linear(2 * out_size, 1)\n",
        "\n",
        "    def forward(self, H, X, X_edges):\n",
        "        Z = self.P(X)\n",
        "        Z_edges = self.P(X_edges)\n",
        "        # print(\"H\",H.shape) # 2708, 2708\n",
        "        # print(\"H.row,H.col\",H.row.shape,H.col.shape) # H.row,H.col tensor([   0,    0,    0,  ..., 2707, 2707, 2707]) tensor([   0,  633, 1862,  ..., 1473, 2706, 2707]) # [13264], [13264]\n",
        "        # print(\"Z[H.row], Z_edges[H.col]\",Z[H.row], Z_edges[H.col].shape) # [13264, 16], [13264, 16]\n",
        "        print(Z,Z_edges.shape) # [2708, 16], [2708, 16]\n",
        "\n",
        "        sim = self.a(torch.cat([Z[H.row], Z_edges[H.col]], 1))\n",
        "        sim = F.leaky_relu(sim, 0.2).squeeze(1)\n",
        "        # Reassign the hypergraph new weights.\n",
        "        H_att = dglsp.val_like(H, sim)\n",
        "        H_att = H_att.softmax()\n",
        "        return hypergraph_laplacian(H_att) @ Z\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_size, out_size, hidden_size=16):\n",
        "        super().__init__()\n",
        "        self.layer1 = HypergraphAttention(in_size, hidden_size)\n",
        "        self.layer2 = HypergraphAttention(hidden_size, out_size)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        Z = self.layer1(H, X, X)\n",
        "        Z = F.elu(Z)\n",
        "        Z = self.layer2(H, Z, Z)\n",
        "        return Z\n",
        "\n",
        "\n",
        "def train(model, optimizer, H, X, Y, train_mask):\n",
        "    model.train()\n",
        "    Y_hat = model(H, X)\n",
        "    loss = F.cross_entropy(Y_hat[train_mask], Y[train_mask])\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def evaluate(model, H, X, Y, val_mask, test_mask, num_classes):\n",
        "    model.eval()\n",
        "    Y_hat = model(H, X)\n",
        "    val_acc = accuracy(Y_hat[val_mask], Y[val_mask])\n",
        "    test_acc = accuracy(Y_hat[test_mask], Y[test_mask])\n",
        "    return val_acc, test_acc\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    dataset = CoraGraphDataset()\n",
        "    graph = dataset[0]\n",
        "    indices = torch.stack(graph.edges())\n",
        "    H = dglsp.spmatrix(indices)\n",
        "    H = H + dglsp.identity(H.shape)\n",
        "    X = graph.ndata[\"feat\"]\n",
        "    Y = graph.ndata[\"label\"]\n",
        "    train_mask = graph.ndata[\"train_mask\"]\n",
        "    val_mask = graph.ndata[\"val_mask\"]\n",
        "    test_mask = graph.ndata[\"test_mask\"]\n",
        "    return H, X, Y, dataset.num_classes, train_mask, val_mask, test_mask\n",
        "\n",
        "\n",
        "H, X, Y, num_classes, train_mask, val_mask, test_mask = load_data()\n",
        "model = Net(X.shape[1], num_classes)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "with tqdm.trange(2) as tq:\n",
        "    for epoch in tq:\n",
        "        loss = train(model, optimizer, H, X, Y, train_mask)\n",
        "        val_acc, test_acc = evaluate(\n",
        "            model, H, X, Y, val_mask, test_mask, num_classes\n",
        "        )\n",
        "        tq.set_postfix(\n",
        "            {\n",
        "                \"Loss\": f\"{loss:.5f}\",\n",
        "                \"Val acc\": f\"{val_acc:.5f}\",\n",
        "                \"Test acc\": f\"{test_acc:.5f}\",\n",
        "            },\n",
        "            refresh=False,\n",
        "        )\n",
        "\n",
        "print(f\"Test acc: {test_acc:.3f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/hypergraph/blob/main/hgnn_list.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title head\n",
        "# https://colab.research.google.com/github/dmlc/dgl/blob/master/notebooks/sparse/hgnn.ipynb\n",
        "# https://github.com/dmlc/dgl/blob/master/notebooks/sparse/hgnn.ipynb\n",
        "# https://github.com/dmlc/dgl/blob/master/examples/sparse/hgnn.py\n",
        "import torch\n",
        "\n",
        "cite=torch.Tensor([[0, 1, 2, 2, 2, 2, 3, 4, 5, 5, 5, 5, 6, 7, 7, 8, 8, 9, 9, 10],\n",
        "                    [0, 0, 0, 1, 3, 4, 2, 1, 0, 2, 3, 4, 2, 1, 3, 1, 3, 2, 4, 4]])\n",
        "H = torch.sparse_coo_tensor(indices=cite, values=torch.ones(cite.shape[1]),).coalesce()\n",
        "# uncoalesced tensors, may be duplicate coords in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries\n",
        "# vert _ is in hyperedge _\n",
        "print(H.to_dense()) # cols: hyperedges ; rows: verts\n",
        "\n",
        "# node_degrees = H.sum(1)\n",
        "# print(\"Node degrees\", node_degrees)\n",
        "# hyperedge_degrees = H.sum(0)\n",
        "# print(\"Hyperedge degrees\", hyperedge_degrees.values())\n"
      ],
      "metadata": {
        "id": "__2tKqL0eaB0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Hypergraph Neural Network (HGNN) Layer\n",
        "\n",
        "The [HGNN layer](https://arxiv.org/pdf/1809.09401.pdf) is defined as:\n",
        "\n",
        "$$f(X^{(l)}, H; W^{(l)}) = \\sigma(L X^{(l)} W^{(l)})$$$$L = D_v^{-1/2} H B D_e^{-1} H^\\top D_v^{-1/2}$$\n",
        "\n",
        "where\n",
        "\n",
        "* $H \\in \\mathbb{R}^{N \\times M}$ is the incidence matrix of hypergraph with $N$ nodes and $M$ hyperedges.\n",
        "* $D_v \\in \\mathbb{R}^{N \\times N}$ is a diagonal matrix representing node degrees, whose $i$-th diagonal element is $\\sum_{j=1}^M H_{ij}$.\n",
        "* $D_e \\in \\mathbb{R}^{M \\times M}$ is a diagonal matrix representing hyperedge degrees, whose $j$-th diagonal element is $\\sum_{i=1}^N H_{ij}$.\n",
        "* $B \\in \\mathbb{R}^{M \\times M}$ is a diagonal matrix representing the hyperedge weights, whose $j$-th diagonal element is the weight of $j$-th hyperedge.  In our example, $B$ is an identity matrix.\n",
        "\n",
        "The following code builds a two-layer HGNN."
      ],
      "metadata": {
        "id": "7kxrINkVHrAi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qVdpGWcncedo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title requests data\n",
        "import requests\n",
        "url = 'https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz'\n",
        "# response = requests.get(url)\n",
        "open(\"cora.tgz\", \"wb\").write(response.content)\n",
        "\n",
        "import tarfile # os, sys,\n",
        "tar = tarfile.open('cora.tgz', 'r')\n",
        "tar.extractall('/content')\n",
        "\n",
        "import torch\n",
        "\n",
        "content = open(\"cora/cora.content\", \"r\")\n",
        "# print(content.read(10000))\n",
        "# paper id, bag of words bool, category 0-6 # all str\n",
        "rlst = content.read().split('\\n')[:-1] # bec last row is ''\n",
        "pid = [] # paper id\n",
        "bow = [] # bag of words\n",
        "cls = [] # classes\n",
        "# category: Case_Based, Genetic_Algorithms, Neural_Networks, Probabilistic_Methods, Reinforcement_Learning, Rule_Learning, Theory\n",
        "category = {'Case_Based':0, 'Genetic_Algorithms':1, 'Neural_Networks':2, 'Probabilistic_Methods':3, 'Reinforcement_Learning':4, 'Rule_Learning':5, 'Theory':6} # cora\n",
        "for r in rlst:\n",
        "    rr=r.split('\\t')\n",
        "    pid.append(int(rr[0]))\n",
        "    bow.append(list(map(float, rr[1:-1]))) # must be float\n",
        "    cls.append(category[rr[-1]])\n",
        "pid=torch.tensor(pid)\n",
        "X=torch.tensor(bow)\n",
        "Y=torch.tensor(cls)\n",
        "num_classes=7\n",
        "\n",
        "# https://stellargraph.readthedocs.io/en/v1.0.0rc1/demos/node-classification/gcn/gcn-cora-node-classification-example.html\n",
        "# The Cora dataset consists of 2708 scientific publications\n",
        "# classified into one of seven classes.\n",
        "# The citation network consists of 5429 links\n",
        "# Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary.\n",
        "# The dictionary consists of 1433 unique words\n",
        "\n",
        "cites = open(\"cora/cora.cites\", \"r\") # cite relation\n",
        "clst = cites.read().split('\\n')[:-1] # bec last row is ''\n",
        "cite = [] #\n",
        "for c in clst:\n",
        "    cc=c.split('\\t')\n",
        "    cite.append([int(cc[0]),int(cc[1])])\n",
        "cite=torch.tensor(cite) # [5429]\n",
        "\n",
        "ukeys = torch.unique(pid)\n",
        "uvals = torch.arange(len(ukeys))\n",
        "udict = dict(zip(ukeys.tolist(), uvals.tolist())) # assign new id to each paper\n",
        "pid = pid.apply_(udict.get)\n",
        "cite = cite.apply_(udict.get)\n",
        "\n",
        "num_v = len(pid)\n",
        "H = torch.sparse_coo_tensor(indices=cite.T, values=torch.ones(cite.shape[0]), size=(num_v, num_v)).coalesce() # size=(2708, 2708), nnz=5429, layout=torch.sparse_coo\n",
        "id = torch.sparse.spdiags(torch.ones(H.shape[0]),torch.tensor(0),H.shape)\n",
        "H = (id + H).coalesce() # each vert got its hyperedge, contain all cited and itself, [2708, 2708], incedence matrix, |V| hyperedges\n",
        "\n",
        "train_mask, val_mask, test_mask = torch.zeros(3, num_v, dtype=torch.bool)\n",
        "train_mask[:140], val_mask[140:640], test_mask[-1000:] = True, True, True # cora mask\n",
        "# print(len(train_mask))\n",
        "# print(train_mask)\n",
        "# H, X, Y, num_classes, train_mask, val_mask, test_mask = load_data()\n",
        "\n",
        "# print(train_mask, val_mask, test_mask)\n",
        "# print(sum(train_mask), sum(val_mask), sum(test_mask)) # 140), (500), (1000)\n",
        "# print(sum(test_mask[-1000:]))\n",
        "# print(len(test_mask)) # 2708\n",
        "# print(train_mask[140])\n",
        "# [:140], [140:640], [-1000:]\n",
        "\n",
        "# print(H.shape, X.shape, Y.shape) # [2708, 2708], [2708, 1433], [2708]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title edge/ incidence list\n",
        "\n",
        "# edic = dict((id, [id]) for id in pid.tolist()) # edge list H(E)={e1,e2,e3}={{A,D},{D,E},{A,B,C}}\n",
        "# idic = dict((id, [id]) for id in pid.tolist()) # incidence list {A:{e1,e3}, B:{e3}, C:{e3}, D:{e1,e2}, E:{e2}}\n",
        "elst = [[id] for id in pid.tolist()] # edge list H(E)={e1,e2,e3}={{A,D},{D,E},{A,B,C}}\n",
        "ilst = [[id] for id in pid.tolist()] # incidence list {A:{e1,e3}, B:{e3}, C:{e3}, D:{e1,e2}, E:{e2}}\n",
        "for a,b in cite.tolist():\n",
        "    elst[a].append(b)\n",
        "    ilst[b].append(a)\n",
        "elst = torch.tensor(elst)\n",
        "ilst = torch.tensor(ilst)\n",
        "# print(elst)\n",
        "# print(ilst)\n",
        "\n"
      ],
      "metadata": {
        "id": "r9BQGQAvNkV4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.rand(10) <= 0.7\n",
        "print(mask)\n",
        "# r=torch.rand(10)\n",
        "# print(r*mask)\n",
        "# print(r[~mask])\n",
        "# print(r[mask==True])\n",
        "# r=[i for i in range(10)]\n",
        "# print([rr for rr, m in zip(r, mask) if m])\n",
        "\n",
        "a=[0,1,2,3]\n",
        "# print(a[mask])\n",
        "print(mask[a])\n",
        "print([aa for aa, m in zip(a, mask[a]) if m])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWmYEkaTWlOX",
        "outputId": "5b3f0356-8fbb-4ef7-aee6-60637675ffdd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ True, False, False,  True, False, False,  True,  True,  True, False])\n",
            "tensor([ True, False, False,  True])\n",
            "[0, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title dgl data\n",
        "!pip install dgl\n",
        "\n",
        "# \"co-cite\" relationship: hyperedge includes all the other papers it cited, as well as the paper itself\n",
        "# then incidence mat = incidence mat + id\n",
        "import torch\n",
        "from dgl.data import CoraGraphDataset # https://github.com/dmlc/dgl/blob/master/python/dgl/data/citation_graph.py\n",
        "\n",
        "def load_data():\n",
        "    dataset = CoraGraphDataset()\n",
        "    graph = dataset[0]\n",
        "    indices = torch.stack(graph.edges())\n",
        "    H = torch.sparse_coo_tensor(indices=indices, values=torch.ones(indices.shape[1]),).coalesce()\n",
        "    id = torch.sparse.spdiags(torch.ones(H.shape[0]),torch.tensor(0),H.shape) # torch.eye(H.shape[0])\n",
        "    H = (id + H).coalesce() # each vert got its hyperedge, contain all cited and itself, [2708, 2708], incedence matrix, |V| hyperedges\n",
        "\n",
        "    X = graph.ndata[\"feat\"] #[2708, 1433] num papers, len bag of words\n",
        "    Y = graph.ndata[\"label\"] # [2708], classiifcation 0-6\n",
        "    train_mask = graph.ndata[\"train_mask\"]\n",
        "    val_mask = graph.ndata[\"val_mask\"]\n",
        "    test_mask = graph.ndata[\"test_mask\"]\n",
        "    return H, X, Y, dataset.num_classes, train_mask, val_mask, test_mask\n",
        "\n",
        "H, X, Y, num_classes, train_mask, val_mask, test_mask = load_data()\n",
        "# print(H.shape, X.shape, Y.shape) # [2708, 2708], [2708, 1433], [2708]\n"
      ],
      "metadata": {
        "id": "qI0j1J9pwTFg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title HMPNN elst, ilst\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# Vert msg = fv(vert ebd)\n",
        "# Edge msg = fw(edge emb, Sum Vert msgs)\n",
        "# Vert emb1 = gv(vert emb, Sum edge msgs)\n",
        "# Edge emb1 = gw(edge emb, Sum Vert msgs)\n",
        "\n",
        "class MsgPass(nn.Module):\n",
        "    def __init__(self, vembdim, eembdim, vmsgdim, emsgdim):\n",
        "        super(MsgPass, self).__init__()\n",
        "        self.h_dim = 4\n",
        "        self.fv = nn.Sequential(\n",
        "            # nn.Linear(vembdim, self.h_dim), nn.ReLU(),\n",
        "            nn.Linear(vembdim, self.h_dim), nn.Sigmoid(),\n",
        "            nn.Linear(self.h_dim, vmsgdim),\n",
        "            # nn.Linear(vembdim, vmsgdim), nn.ReLU(),\n",
        "            # nn.Linear(vembdim, vmsgdim), nn.Sigmoid(),\n",
        "            )\n",
        "        self.fw = nn.Sequential(\n",
        "            # nn.Linear(eembdim+vmsgdim, self.h_dim), nn.ReLU(),\n",
        "            nn.Linear(eembdim+vmsgdim, self.h_dim), nn.Sigmoid(),\n",
        "            nn.Linear(self.h_dim, emsgdim),\n",
        "            # nn.Linear(eembdim+vmsgdim, emsgdim), nn.Sigmoid(),\n",
        "            # nn.Linear(eembdim+vmsgdim, emsgdim), nn.ReLU(),\n",
        "            )\n",
        "        self.gv = nn.Sequential(\n",
        "            # nn.Linear(vembdim+emsgdim, self.h_dim), nn.ReLU(),\n",
        "            nn.Linear(vembdim+emsgdim, self.h_dim), nn.Sigmoid(),\n",
        "            nn.Linear(self.h_dim, vembdim),\n",
        "            # nn.Linear(vembdim+emsgdim, vembdim), nn.ReLU(),\n",
        "            # nn.Linear(vembdim+emsgdim, vembdim), nn.Sigmoid(),\n",
        "            )\n",
        "        self.gw = nn.Sequential(\n",
        "            # nn.Linear(eembdim+vmsgdim, self.h_dim), nn.ReLU(),\n",
        "            nn.Linear(eembdim+vmsgdim, self.h_dim), nn.Sigmoid(),\n",
        "            nn.Linear(self.h_dim, eembdim),\n",
        "            # nn.Linear(eembdim+vmsgdim, eembdim), nn.Sigmoid(),\n",
        "            # nn.Linear(eembdim+vmsgdim, eembdim), nn.ReLU(),\n",
        "            )\n",
        "        self.vmsgdim = vmsgdim\n",
        "        self.emsgdim = emsgdim\n",
        "\n",
        "    def forward(self, vemb, eemb, elst, ilst):\n",
        "        # semsg=torch.stack([torch.mean(emsg[e],0) for e in ilst]) # given v, get all emsgs then aggregate\n",
        "        vmsg = self.fv(vemb)\n",
        "        # vmsg = self.fv(torch.cat((vemb, semsg), 1))\n",
        "        vmsg = F.dropout(F.batch_norm(vmsg,torch.zeros(self.vmsgdim),torch.ones(self.vmsgdim)),p=0.5)\n",
        "\n",
        "        mvemb, meemb, melst, milst = vemb, eemb, elst, ilst\n",
        "        _, meemb, melst, milst = adjdrop(vemb, eemb, elst, ilst, p=0.7)\n",
        "\n",
        "        svmsg=torch.stack([torch.sum(vmsg[v],0) for v in melst]) # given e, get all vmsgs then aggregate\n",
        "        memsg = self.fw(torch.cat((meemb, svmsg), 1))\n",
        "        memsg = F.dropout(F.batch_norm(memsg,torch.zeros(self.emsgdim),torch.ones(self.emsgdim)),p=0.5)\n",
        "\n",
        "        # cannot be mean bec vert in ilst may be isolated, divide by 0 hyperedges\n",
        "        semsg=torch.stack([torch.sum(memsg[e],0) for e in milst]) # given v, get all emsgs then aggregate\n",
        "        vemb1 = self.gv(torch.cat((vemb, semsg), 1))\n",
        "\n",
        "        svmsg=torch.stack([torch.sum(vmsg[v],0) for v in elst]) # given e, get all vmsgs then aggregate\n",
        "        eemb1 = self.gw(torch.cat((eemb, svmsg), 1))\n",
        "        return vemb1, eemb1\n",
        "\n",
        "def ilst_from_elst(elst, n_v=len(ilst)): # generate incidence list from edge list\n",
        "    ilst = [[] for id in range(n_v)]\n",
        "    for e,vs in enumerate(elst):\n",
        "        [ilst[v].append(e) for v in vs]\n",
        "    return ilst\n",
        "\n",
        "def adjdrop(vemb, eemb, elst, ilst, p=0.7): # adjacency dropout, maybe can replace with slicing of sparse tensors if pytorch implements it\n",
        "    mask = torch.rand(len(elst)) >= p # True->keep, throw p\n",
        "    melst = [e for e, m in zip(elst, mask) if m]\n",
        "    meemb = eemb[mask==True]\n",
        "    milst = ilst_from_elst(melst)\n",
        "    return vemb, meemb, melst, milst\n",
        "\n",
        "\n",
        "class HMPNN(nn.Module):\n",
        "    def __init__(self, outdim, vembdim, eembdim):\n",
        "        super(HMPNN, self).__init__()\n",
        "        self.msgpass = MsgPass(vembdim, eembdim, vmsgdim=2, emsgdim=2)\n",
        "        self.lin = nn.Linear(vembdim, outdim)\n",
        "\n",
        "    def forward(self, x, elst=elst, ilst=ilst):\n",
        "        vemb = x\n",
        "        eemb = torch.zeros(len(elst),eembdim)\n",
        "        # vemb, eemb = self.msgpass(vemb, eemb)\n",
        "        vemb, eemb = self.msgpass(vemb, eemb, elst=elst, ilst=ilst)\n",
        "        vemb, eemb = self.msgpass(vemb, eemb, elst=elst, ilst=ilst)\n",
        "        x = self.lin(vemb)\n",
        "        return x\n",
        "\n",
        "num_v,vembdim=X.shape\n",
        "eembdim=2\n",
        "num_classes=7\n",
        "model=HMPNN(num_classes, vembdim, eembdim)\n",
        "yhat = model(X, elst, ilst)\n",
        "# print(yhat.shape) # [2708, 7]\n",
        "\n",
        "# print(len(X[0]))"
      ],
      "metadata": {
        "id": "0pPCFxBcvFsH"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad\n",
        "def hypergraph_laplacian(H):\n",
        "    N,M = H.shape\n",
        "    d_V = H.sum(1).to_dense() # node deg\n",
        "    d_E = H.sum(0).to_dense() # edge deg\n",
        "    D_v_invsqrt = torch.sparse.spdiags(d_V**-0.5,torch.tensor(0),(N,N)) # torch.diag(d_V**-0.5)\n",
        "    D_e_inv = torch.sparse.spdiags(d_E**-1,torch.tensor(0),(M,M)) # torch.diag(d_E**-1)\n",
        "    B = torch.sparse.spdiags(torch.ones(M),torch.tensor(0),(M,M)) # torch.eye(M) # B is id, dim n_edges\n",
        "    return D_v_invsqrt @ H @ B @ D_e_inv @ H.T @ D_v_invsqrt # Laplacian\n",
        "\n",
        "class HGNN(nn.Module): # https://github.com/dmlc/dgl/blob/master/notebooks/sparse/hgnn.ipynb\n",
        "    def __init__(self, H, in_size, out_size, hidden_dims=16):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(in_size, hidden_dims)\n",
        "        self.W2 = nn.Linear(hidden_dims, out_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.L = hypergraph_laplacian(H)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        X = self.L @ self.W1(self.dropout(X))\n",
        "        X = F.relu(X)\n",
        "        X = self.L @ self.W2(self.dropout(X))\n",
        "        return X\n",
        "\n",
        "# Hypergraph Convolution and Hypergraph Attention https://arxiv.org/pdf/1901.08150.pdf\n",
        "class HypergraphAttention(nn.Module): # https://github.com/dmlc/dgl/blob/master/examples/sparse/hypergraphatt.py\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super().__init__()\n",
        "        self.P = nn.Linear(in_size, out_size)\n",
        "        self.a = nn.Linear(2 * out_size, 1)\n",
        "\n",
        "    def forward(self, H, X, X_edges):\n",
        "        Z = self.P(X)\n",
        "        Z_edges = self.P(X_edges)\n",
        "        # sim = self.a(torch.cat([Z[H.indices()[0]], Z_edges[H.indices()[1]]], 1))\n",
        "        sim = self.a(torch.cat([Z[H.coalesce().indices()[0]], Z_edges[H.coalesce().indices()[1]]], 1))\n",
        "        sim = F.leaky_relu(sim, 0.2).squeeze(1)\n",
        "        print(H.shape,sim.shape) # [2708, 2708]) torch.Size([5429] og[13264]\n",
        "        # H_att = torch.sparse_coo_tensor(indices=H.indices(), values=sim,).coalesce()\n",
        "        H_att = torch.sparse_coo_tensor(indices=H.coalesce().indices(), values=sim,).coalesce()\n",
        "        H_att = torch.sparse.softmax(H_att,1)\n",
        "        # print(H.shape,H_att.shape) # [2708, 2708], [1898, 2708]\n",
        "        # print(hypergraph_laplacian(H_att).shape, Z.shape)\n",
        "        return hypergraph_laplacian(H_att) @ Z\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_size, out_size, hidden_size=16):\n",
        "        super().__init__()\n",
        "        self.layer1 = HypergraphAttention(in_size, hidden_size)\n",
        "        self.layer2 = HypergraphAttention(hidden_size, out_size)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        Z = self.layer1(H, X, X)\n",
        "        Z = F.elu(Z)\n",
        "        Z = self.layer2(H, Z, Z)\n",
        "        return Z\n",
        "\n"
      ],
      "metadata": {
        "id": "58WnPtPvT2mx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train/ eval\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train(model, optimizer, H, X, Y, train_mask):\n",
        "    model.train()\n",
        "    Y_hat = model(H, X)\n",
        "    loss = F.cross_entropy(Y_hat[train_mask], Y[train_mask]) # loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def accuracy(yhat, y): return (yhat.argmax(1) == y).type(torch.float).sum().item()/y.shape[0]\n",
        "\n",
        "def evaluate(model, H, X, Y, val_mask, test_mask):\n",
        "    model.eval()\n",
        "    Y_hat = model(H, X) # model(X)\n",
        "    val_acc = accuracy(Y_hat[val_mask], Y[val_mask])\n",
        "    test_acc = accuracy(Y_hat[test_mask], Y[test_mask])\n",
        "    return val_acc, test_acc\n",
        "\n",
        "\n",
        "def trainl(model, optimizer, elst, ilst, X, Y, train_mask):\n",
        "    model.train()\n",
        "    Y_hat = model(X,elst, ilst)\n",
        "    l,r=0,4\n",
        "    # print(Y_hat[train_mask][l:r], Y[train_mask][l:r])\n",
        "    loss = F.cross_entropy(Y_hat[train_mask], Y[train_mask]) # loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluatel(model, elst, ilst, X, Y, val_mask, test_mask):\n",
        "    model.eval()\n",
        "    Y_hat = model(X,elst, ilst) # model(X)\n",
        "    val_acc = accuracy(Y_hat[val_mask], Y[val_mask])\n",
        "    test_acc = accuracy(Y_hat[test_mask], Y[test_mask])\n",
        "    return val_acc, test_acc\n",
        "\n",
        "# val_acc, test_acc = evaluatel(model, elst, ilst, X, Y, val_mask, test_mask)\n",
        "# val_acc, test_acc = evaluatel(model, elst, ilst, X, Y, val_mask, train_mask)\n",
        "# print(val_acc, test_acc)\n",
        "\n"
      ],
      "metadata": {
        "id": "IfEc6JRXwHPt",
        "cellView": "form"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title run\n",
        "\n",
        "# model = HGNN(H, X.shape[1], num_classes) # hg conv\n",
        "# model = Net(X.shape[1], num_classes) # hg att\n",
        "model = HMPNN(num_classes, vembdim, eembdim) # hg msg pass\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # 0.001\n",
        "# optimizer.lr=0.01\n",
        "\n",
        "for epoch in range(200):\n",
        "    # loss = train(model, optimizer, H, X, Y, train_mask)\n",
        "    # val_acc, test_acc = evaluate(model, H, X, Y, val_mask, test_mask)\n",
        "    loss = trainl(model, optimizer, elst, ilst, X, Y, train_mask)\n",
        "    val_acc, test_acc = evaluatel(model, elst, ilst, X, Y, val_mask, test_mask)\n",
        "    print(f\"test loss: {loss:.5f}, Val acc: {val_acc:.5f}, Test acc: {test_acc:.5f}\")\n",
        "\n",
        "# attn 200epoch 1min test loss: 0.00641, Val acc: 0.76000, Test acc: 0.74700\n",
        "# hmpnn relu 7 epoch test loss: 0.00779, Val acc: 0.53800, Test acc: 0.47500\n",
        "# hmpnn relu 200 epoch test loss: 0.00000, Val acc: 0.43400, Test acc: 0.41000\n",
        "# hmpnn 2lin relu 200 epoch test loss: 0.02837, Val acc: 0.44000, Test acc: 0.38000\n",
        "# hmpnn sigmoid test loss: 0.00064, Val acc: 0.53000, Test acc: 0.45100\n",
        "# 2hmpnn sigmoid test loss: 1.81603, Val acc: 0.35000, Test acc: 0.29500\n",
        "# 2hmpnn relu test loss: 0.00000, Val acc: 0.42800, Test acc: 0.45100\n",
        "# 2hmpnn 2lin relu 200 epoch test loss: 1.81581, Val acc: 0.35000, Test acc: 0.29500\n",
        "# 2hmpnn 2lin sigmoid 200 epoch test loss: 1.81585, Val acc: 0.35000, Test acc: 0.29500\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_m-1wJk4LZFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### trash"
      ],
      "metadata": {
        "id": "FoWvM32-vHcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hmpnn with H rej\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# Vert msg = fv(vert ebd)\n",
        "# Edge msg = fw(edge emb, Sum Vert msgs)\n",
        "# Vert emb1 = gv(vert emb, Sum edge msgs)\n",
        "# Edge emb1 = gw(edge emb, Sum Vert msgs)\n",
        "\n",
        "class MsgPass(nn.Module):\n",
        "    def __init__(self, vembdim, eembdim, vmsgdim, emsgdim):\n",
        "        super(MsgPass, self).__init__()\n",
        "        self.fv = nn.Sequential(\n",
        "            # nn.Linear(vembdim, 64), nn.ReLU(),\n",
        "            nn.Linear(vembdim, vmsgdim), nn.ReLU(),\n",
        "            # nn.Linear(64, vmsgdim),\n",
        "            )\n",
        "        self.fw = nn.Sequential(\n",
        "            # nn.Linear(eembdim+vmsgdim, 64), nn.ReLU(),\n",
        "            nn.Linear(eembdim+vmsgdim, emsgdim), nn.ReLU(),\n",
        "            # nn.Linear(64, emsgdim),\n",
        "            )\n",
        "        self.gv = nn.Sequential(\n",
        "            # nn.Linear(vembdim+emsgdim, 64), nn.ReLU(),\n",
        "            nn.Linear(vembdim+emsgdim, vembdim), nn.ReLU(),\n",
        "            # nn.Linear(64, vembdim),\n",
        "            )\n",
        "        self.gw = nn.Sequential(\n",
        "            # nn.Linear(eembdim+vmsgdim, 64), nn.ReLU(),\n",
        "            nn.Linear(eembdim+vmsgdim, eembdim), nn.ReLU(),\n",
        "            # nn.Linear(64, eembdim),\n",
        "            )\n",
        "\n",
        "    def forward(self, H, vemb, eemb):\n",
        "        # semsg=torch.stack([torch.mean(emsg[e],0) for e in ilst]) # given v, get all emsgs then aggregate\n",
        "        vmsg = self.fv(vemb)\n",
        "\n",
        "        # vmsg = self.fv(torch.cat((vemb, semsg), 1))\n",
        "        # print(vmsg.shape)\n",
        "        mask = torch.rand(len(elst)) <= 0.7\n",
        "        # melst = [e if m for e, m in zip(elst, mask)]\n",
        "        melst = [e if m else [] for e, m in zip(elst, mask)]\n",
        "        print(melst)\n",
        "        # svmsg=torch.stack([torch.mean(vmsg[v],0) for v in elst]) # given e, get all vmsgs then aggregate\n",
        "        svmsg=torch.stack([torch.mean(vmsg[v],0) for v in melst]) # given e, get all vmsgs then aggregate\n",
        "        print(svmsg.shape)\n",
        "        emsg = self.fw(torch.cat((eemb, svmsg), 1))\n",
        "        # emsg = self.fw(torch.cat((eemb[mask==True], svmsg), 1))\n",
        "        # nn.BatchNorm1d(dim) # nn.Dropout(p=0.7)\n",
        "\n",
        "        semsg=torch.stack([torch.mean(emsg[e],0) for e in ilst]) # given v, get all emsgs then aggregate\n",
        "        # semsg=torch.stack([torch.mean(emsg[e],0) if mask[i] else 0 for i,e in enumerate(ilst)]) # given v, get all emsgs then aggregate\n",
        "        vemb1 = self.gv(torch.cat((vemb, semsg), 1))\n",
        "        # nn.BatchNorm1d(dim) # nn.Dropout(p=0.7)\n",
        "\n",
        "        # svmsg=torch.stack([torch.mean(vmsg[v],0) for v in elst]) # given e, get all vmsgs then aggregate\n",
        "        # [aa for aa, m in zip(a, mask[a]) if m]\n",
        "        svmsg=torch.stack([torch.mean(vmsg[v],0) for v in melst]) # given e, get all vmsgs then aggregate\n",
        "        eemb1 = self.gw(torch.cat((eemb, svmsg), 1))\n",
        "        print(svmsg.shape,eemb1.shape)\n",
        "        return vemb1, eemb1\n",
        "\n",
        "class HMPNN(nn.Module):\n",
        "    def __init__(self, outdim, vembdim, eembdim):\n",
        "        super(HMPNN, self).__init__()\n",
        "        self.msgpass = MsgPass(vembdim, eembdim, vmsgdim=2, emsgdim=2)\n",
        "        self.lin = nn.Linear(vembdim, outdim)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        vemb = x\n",
        "        eemb = torch.zeros(len(elst),eembdim)\n",
        "        # vemb, eemb = self.msgpass(vemb, eemb)\n",
        "        vemb, eemb = self.msgpass(vemb, eemb, elst=elst, ilst=ilst)\n",
        "        x = self.lin(vemb)\n",
        "        return x\n",
        "\n",
        "num_v,vembdim=X.shape\n",
        "eembdim=2\n",
        "num_classes=7\n",
        "model=HMPNN(num_classes, vembdim, eembdim)\n",
        "model(X, elst, ilst)\n",
        "\n",
        "\n",
        "Y_hat = model(H, X)\n",
        "\n",
        "\n",
        "# print(len(X[0]))"
      ],
      "metadata": {
        "id": "k8ILlNwnU3i0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title old og hg attn\n",
        "Hypergraph Convolution and Hypergraph Attention\n",
        "(https://arxiv.org/pdf/1901.08150.pdf).\n",
        "import argparse\n",
        "import dgl.sparse as dglsp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "from dgl.data import CoraGraphDataset\n",
        "def accuracy(yhat, y): return (yhat.argmax(1) == y).type(torch.float).sum().item()/y.shape[0]\n",
        "\n",
        "\n",
        "def hypergraph_laplacian(H):\n",
        "    ###########################################################\n",
        "    # (HIGHLIGHT) Compute the Laplacian with Sparse Matrix API\n",
        "    ###########################################################\n",
        "    d_V = H.sum(1)  # node degree\n",
        "    d_E = H.sum(0)  # edge degree\n",
        "    n_edges = d_E.shape[0]\n",
        "    D_V_invsqrt = dglsp.diag(d_V**-0.5)  # D_V ** (-1/2)\n",
        "    D_E_inv = dglsp.diag(d_E**-1)  # D_E ** (-1)\n",
        "    W = dglsp.identity((n_edges, n_edges))\n",
        "    return D_V_invsqrt @ H @ W @ D_E_inv @ H.T @ D_V_invsqrt\n",
        "\n",
        "\n",
        "class HypergraphAttention(nn.Module):\n",
        "    \"\"\"Hypergraph Attention module as in the paper\n",
        "    `Hypergraph Convolution and Hypergraph Attention\n",
        "    <https://arxiv.org/pdf/1901.08150.pdf>`_.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.P = nn.Linear(in_size, out_size)\n",
        "        self.a = nn.Linear(2 * out_size, 1)\n",
        "\n",
        "    def forward(self, H, X, X_edges):\n",
        "        Z = self.P(X)\n",
        "        Z_edges = self.P(X_edges)\n",
        "        # print(\"H\",H.shape) # 2708, 2708\n",
        "        # print(\"H.row,H.col\",H.row.shape,H.col.shape) # H.row,H.col tensor([   0,    0,    0,  ..., 2707, 2707, 2707]) tensor([   0,  633, 1862,  ..., 1473, 2706, 2707]) # [13264], [13264]\n",
        "        # print(\"Z[H.row], Z_edges[H.col]\",Z[H.row], Z_edges[H.col].shape) # [13264, 16], [13264, 16]\n",
        "        print(Z,Z_edges.shape) # [2708, 16], [2708, 16]\n",
        "\n",
        "        sim = self.a(torch.cat([Z[H.row], Z_edges[H.col]], 1))\n",
        "        sim = F.leaky_relu(sim, 0.2).squeeze(1)\n",
        "        # Reassign the hypergraph new weights.\n",
        "        H_att = dglsp.val_like(H, sim)\n",
        "        H_att = H_att.softmax()\n",
        "        return hypergraph_laplacian(H_att) @ Z\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_size, out_size, hidden_size=16):\n",
        "        super().__init__()\n",
        "        self.layer1 = HypergraphAttention(in_size, hidden_size)\n",
        "        self.layer2 = HypergraphAttention(hidden_size, out_size)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        Z = self.layer1(H, X, X)\n",
        "        Z = F.elu(Z)\n",
        "        Z = self.layer2(H, Z, Z)\n",
        "        return Z\n",
        "\n",
        "\n",
        "def train(model, optimizer, H, X, Y, train_mask):\n",
        "    model.train()\n",
        "    Y_hat = model(H, X)\n",
        "    loss = F.cross_entropy(Y_hat[train_mask], Y[train_mask])\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def evaluate(model, H, X, Y, val_mask, test_mask, num_classes):\n",
        "    model.eval()\n",
        "    Y_hat = model(H, X)\n",
        "    val_acc = accuracy(Y_hat[val_mask], Y[val_mask])\n",
        "    test_acc = accuracy(Y_hat[test_mask], Y[test_mask])\n",
        "    return val_acc, test_acc\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    dataset = CoraGraphDataset()\n",
        "    graph = dataset[0]\n",
        "    indices = torch.stack(graph.edges())\n",
        "    H = dglsp.spmatrix(indices)\n",
        "    H = H + dglsp.identity(H.shape)\n",
        "    X = graph.ndata[\"feat\"]\n",
        "    Y = graph.ndata[\"label\"]\n",
        "    train_mask = graph.ndata[\"train_mask\"]\n",
        "    val_mask = graph.ndata[\"val_mask\"]\n",
        "    test_mask = graph.ndata[\"test_mask\"]\n",
        "    return H, X, Y, dataset.num_classes, train_mask, val_mask, test_mask\n",
        "\n",
        "\n",
        "H, X, Y, num_classes, train_mask, val_mask, test_mask = load_data()\n",
        "model = Net(X.shape[1], num_classes)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "with tqdm.trange(2) as tq:\n",
        "    for epoch in tq:\n",
        "        loss = train(model, optimizer, H, X, Y, train_mask)\n",
        "        val_acc, test_acc = evaluate(\n",
        "            model, H, X, Y, val_mask, test_mask, num_classes\n",
        "        )\n",
        "        tq.set_postfix(\n",
        "            {\n",
        "                \"Loss\": f\"{loss:.5f}\",\n",
        "                \"Val acc\": f\"{val_acc:.5f}\",\n",
        "                \"Test acc\": f\"{test_acc:.5f}\",\n",
        "            },\n",
        "            refresh=False,\n",
        "        )\n",
        "\n",
        "print(f\"Test acc: {test_acc:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sTRlHcQtEFXF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
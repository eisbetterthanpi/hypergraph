{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/hypergraph/blob/main/dgl_hgnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title head\n",
        "# https://colab.research.google.com/github/dmlc/dgl/blob/master/notebooks/sparse/hgnn.ipynb\n",
        "# https://github.com/dmlc/dgl/blob/master/notebooks/sparse/hgnn.ipynb\n",
        "# https://github.com/dmlc/dgl/blob/master/examples/sparse/hgnn.py\n",
        "import torch\n",
        "\n",
        "cite=torch.Tensor([[0, 1, 2, 2, 2, 2, 3, 4, 5, 5, 5, 5, 6, 7, 7, 8, 8, 9, 9, 10],\n",
        "                    [0, 0, 0, 1, 3, 4, 2, 1, 0, 2, 3, 4, 2, 1, 3, 1, 3, 2, 4, 4]])\n",
        "H = torch.sparse_coo_tensor(indices=cite, values=torch.ones(cite.shape[1]),).coalesce()\n",
        "# vert _ is in hyperedge _\n",
        "# print(H.to_dense()) # cols: hyperedges ; rows: verts\n",
        "\n",
        "# node_degrees = H.sum(1)\n",
        "# print(\"Node degrees\", node_degrees)\n",
        "# hyperedge_degrees = H.sum(0)\n",
        "# print(\"Hyperedge degrees\", hyperedge_degrees.values())\n"
      ],
      "metadata": {
        "id": "__2tKqL0eaB0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Hypergraph Neural Network (HGNN) Layer\n",
        "\n",
        "The [HGNN layer](https://arxiv.org/pdf/1809.09401.pdf) is defined as:\n",
        "\n",
        "$$f(X^{(l)}, H; W^{(l)}) = \\sigma(L X^{(l)} W^{(l)})$$$$L = D_v^{-1/2} H B D_e^{-1} H^\\top D_v^{-1/2}$$\n",
        "\n",
        "where\n",
        "\n",
        "* $H \\in \\mathbb{R}^{N \\times M}$ is the incidence matrix of hypergraph with $N$ nodes and $M$ hyperedges.\n",
        "* $D_v \\in \\mathbb{R}^{N \\times N}$ is a diagonal matrix representing node degrees, whose $i$-th diagonal element is $\\sum_{j=1}^M H_{ij}$.\n",
        "* $D_e \\in \\mathbb{R}^{M \\times M}$ is a diagonal matrix representing hyperedge degrees, whose $j$-th diagonal element is $\\sum_{i=1}^N H_{ij}$.\n",
        "* $B \\in \\mathbb{R}^{M \\times M}$ is a diagonal matrix representing the hyperedge weights, whose $j$-th diagonal element is the weight of $j$-th hyperedge.  In our example, $B$ is an identity matrix.\n",
        "\n",
        "The following code builds a two-layer HGNN."
      ],
      "metadata": {
        "id": "7kxrINkVHrAi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "qVdpGWcncedo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title requests data\n",
        "import requests\n",
        "url = 'https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz'\n",
        "response = requests.get(url)\n",
        "open(\"cora.tgz\", \"wb\").write(response.content)\n",
        "\n",
        "import tarfile # os, sys,\n",
        "tar = tarfile.open('cora.tgz', 'r')\n",
        "tar.extractall('/content')\n",
        "\n",
        "import torch\n",
        "\n",
        "content = open(\"cora/cora.content\", \"r\")\n",
        "# print(content.read(10000))\n",
        "# paper id, bag of words bool, category 0-6 # all str\n",
        "rlst = content.read().split('\\n')[:-1] # bec last row is ''\n",
        "pid = [] # paper id\n",
        "bow = [] # bag of words\n",
        "cls = [] # classes\n",
        "# category: Case_Based, Genetic_Algorithms, Neural_Networks, Probabilistic_Methods, Reinforcement_Learning, Rule_Learning, Theory\n",
        "category = {'Case_Based':0, 'Genetic_Algorithms':1, 'Neural_Networks':2, 'Probabilistic_Methods':3, 'Reinforcement_Learning':4, 'Rule_Learning':5, 'Theory':6} # cora\n",
        "for r in rlst:\n",
        "    rr=r.split('\\t')\n",
        "    pid.append(int(rr[0]))\n",
        "    bow.append(list(map(float, rr[1:-1]))) # must be float\n",
        "    cls.append(category[rr[-1]])\n",
        "pid=torch.tensor(pid)\n",
        "X=torch.tensor(bow)\n",
        "Y=torch.tensor(cls)\n",
        "num_classes=7\n",
        "\n",
        "# https://stellargraph.readthedocs.io/en/v1.0.0rc1/demos/node-classification/gcn/gcn-cora-node-classification-example.html\n",
        "# The Cora dataset consists of 2708 scientific publications\n",
        "# classified into one of seven classes.\n",
        "# The citation network consists of 5429 links\n",
        "# Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary.\n",
        "# The dictionary consists of 1433 unique words\n",
        "\n",
        "cites = open(\"cora/cora.cites\", \"r\") # cite relation\n",
        "clst = cites.read().split('\\n')[:-1] # bec last row is ''\n",
        "cite = [] #\n",
        "for c in clst:\n",
        "    cc=c.split('\\t')\n",
        "    cite.append([int(cc[0]),int(cc[1])])\n",
        "cite=torch.tensor(cite) # [5429]\n",
        "\n",
        "ukeys = torch.unique(pid)\n",
        "uvals = torch.arange(len(ukeys))\n",
        "udict = dict(zip(ukeys.tolist(), uvals.tolist())) # assign new id to each paper\n",
        "pid = pid.apply_(udict.get)\n",
        "cite = cite.apply_(udict.get)\n",
        "\n",
        "num_v = len(pid)\n",
        "H = torch.sparse_coo_tensor(indices=cite.T, values=torch.ones(cite.shape[0]), size=(num_v, num_v)).coalesce() # size=(2708, 2708), nnz=5429, layout=torch.sparse_coo\n",
        "id = torch.sparse.spdiags(torch.ones(H.shape[0]),torch.tensor(0),H.shape)\n",
        "H = (id + H).coalesce() # each vert got its hyperedge, contain all cited and itself, [2708, 2708], incedence matrix, |V| hyperedges\n",
        "\n",
        "train_mask, val_mask, test_mask = torch.zeros(3, num_v, dtype=torch.bool)\n",
        "train_mask[:140], val_mask[140:640], test_mask[-1000:] = True, True, True # cora mask\n",
        "# print(len(train_mask))\n",
        "# print(train_mask)\n",
        "# H, X, Y, num_classes, train_mask, val_mask, test_mask = load_data()\n",
        "\n",
        "# print(train_mask, val_mask, test_mask)\n",
        "# print(sum(train_mask), sum(val_mask), sum(test_mask)) # 140), (500), (1000)\n",
        "# print(sum(test_mask[-1000:]))\n",
        "# print(len(test_mask)) # 2708\n",
        "# print(train_mask[140])\n",
        "# [:140], [140:640], [-1000:]\n",
        "\n",
        "# print(H.shape, X.shape, Y.shape) # [2708, 2708], [2708, 1433], [2708]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title dgl data\n",
        "!pip install dgl\n",
        "\n",
        "# \"co-cite\" relationship: hyperedge includes all the other papers it cited, as well as the paper itself\n",
        "# then incidence mat = incidence mat + id\n",
        "import torch\n",
        "from dgl.data import CoraGraphDataset # https://github.com/dmlc/dgl/blob/master/python/dgl/data/citation_graph.py\n",
        "\n",
        "def load_data():\n",
        "    dataset = CoraGraphDataset()\n",
        "    graph = dataset[0]\n",
        "    indices = torch.stack(graph.edges())\n",
        "    H = torch.sparse_coo_tensor(indices=indices, values=torch.ones(indices.shape[1]),).coalesce()\n",
        "    id = torch.sparse.spdiags(torch.ones(H.shape[0]),torch.tensor(0),H.shape) # torch.eye(H.shape[0])\n",
        "    H = (id + H).coalesce() # each vert got its hyperedge, contain all cited and itself, [2708, 2708], incedence matrix, |V| hyperedges\n",
        "\n",
        "    X = graph.ndata[\"feat\"] #[2708, 1433] num papers, len bag of words\n",
        "    Y = graph.ndata[\"label\"] # [2708], classiifcation 0-6\n",
        "    train_mask = graph.ndata[\"train_mask\"]\n",
        "    val_mask = graph.ndata[\"val_mask\"]\n",
        "    test_mask = graph.ndata[\"test_mask\"]\n",
        "    return H, X, Y, dataset.num_classes, train_mask, val_mask, test_mask\n",
        "\n",
        "H, X, Y, num_classes, train_mask, val_mask, test_mask = load_data()\n",
        "# print(H.shape, X.shape, Y.shape) # [2708, 2708], [2708, 1433], [2708]\n"
      ],
      "metadata": {
        "id": "qI0j1J9pwTFg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad\n",
        "def hypergraph_laplacian(H):\n",
        "    N,M = H.shape\n",
        "    d_V = H.sum(1).to_dense() # node deg\n",
        "    d_E = H.sum(0).to_dense() # edge deg\n",
        "    D_v_invsqrt = torch.sparse.spdiags(d_V**-0.5,torch.tensor(0),(N,N)) # torch.diag(d_V**-0.5)\n",
        "    D_e_inv = torch.sparse.spdiags(d_E**-1,torch.tensor(0),(M,M)) # torch.diag(d_E**-1)\n",
        "    B = torch.sparse.spdiags(torch.ones(M),torch.tensor(0),(M,M)) # torch.eye(M) # B is id, dim n_edges\n",
        "    return D_v_invsqrt @ H @ B @ D_e_inv @ H.T @ D_v_invsqrt # Laplacian\n",
        "\n",
        "class HGNN(nn.Module): # https://github.com/dmlc/dgl/blob/master/notebooks/sparse/hgnn.ipynb\n",
        "    def __init__(self, H, in_size, out_size, hidden_dims=16):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(in_size, hidden_dims)\n",
        "        self.W2 = nn.Linear(hidden_dims, out_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.L = hypergraph_laplacian(H)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        X = self.L @ self.W1(self.dropout(X))\n",
        "        X = F.relu(X)\n",
        "        X = self.L @ self.W2(self.dropout(X))\n",
        "        return X\n",
        "\n",
        "# Hypergraph Convolution and Hypergraph Attention https://arxiv.org/pdf/1901.08150.pdf\n",
        "class HypergraphAttention(nn.Module): # https://github.com/dmlc/dgl/blob/master/examples/sparse/hypergraphatt.py\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super().__init__()\n",
        "        self.P = nn.Linear(in_size, out_size)\n",
        "        self.a = nn.Linear(2 * out_size, 1)\n",
        "\n",
        "    def forward(self, H, X, X_edges):\n",
        "        Z = self.P(X)\n",
        "        Z_edges = self.P(X_edges)\n",
        "        sim = self.a(torch.cat([Z[H.indices()[0]], Z_edges[H.indices()[1]]], 1))\n",
        "        sim = F.leaky_relu(sim, 0.2).squeeze(1)\n",
        "        print(H.shape,sim.shape) # [2708, 2708]) torch.Size([5429] og[13264]\n",
        "        H_att = torch.sparse_coo_tensor(indices=H.indices(), values=sim,).coalesce()\n",
        "        H_att = torch.sparse.softmax(H_att,1)\n",
        "        # print(H.shape,H_att.shape) # [2708, 2708], [1898, 2708]\n",
        "        # print(hypergraph_laplacian(H_att).shape, Z.shape)\n",
        "        return hypergraph_laplacian(H_att) @ Z\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_size, out_size, hidden_size=16):\n",
        "        super().__init__()\n",
        "        self.layer1 = HypergraphAttention(in_size, hidden_size)\n",
        "        self.layer2 = HypergraphAttention(hidden_size, out_size)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        Z = self.layer1(H, X, X)\n",
        "        Z = F.elu(Z)\n",
        "        Z = self.layer2(H, Z, Z)\n",
        "        return Z\n",
        "\n"
      ],
      "metadata": {
        "id": "58WnPtPvT2mx",
        "cellView": "form"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train/ eval\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train(model, optimizer, H, X, Y, train_mask):\n",
        "    model.train()\n",
        "    Y_hat = model(H, X)\n",
        "    loss = F.cross_entropy(Y_hat[train_mask], Y[train_mask]) # loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def accuracy(yhat, y): return (yhat.argmax(1) == y).type(torch.float).sum().item()/y.shape[0]\n",
        "\n",
        "def evaluate(model, H, X, Y, val_mask, test_mask):\n",
        "    model.eval()\n",
        "    Y_hat = model(H, X) # model(X)\n",
        "    print(Y_hat.shape,H.shape, X.shape) # [2708, 7], [2708, 2708], [2708, 1433])\n",
        "    val_acc = accuracy(Y_hat[val_mask], Y[val_mask])\n",
        "    test_acc = accuracy(Y_hat[test_mask], Y[test_mask])\n",
        "    return val_acc, test_acc\n"
      ],
      "metadata": {
        "id": "IfEc6JRXwHPt",
        "cellView": "form"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title run\n",
        "\n",
        "# model = HGNN(H, X.shape[1], num_classes) # hg conv\n",
        "model = Net(X.shape[1], num_classes) # hg att\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # 0.001\n",
        "\n",
        "for epoch in range(2):\n",
        "    loss = train(model, optimizer, H, X, Y, train_mask)\n",
        "    val_acc, test_acc = evaluate(model, H, X, Y, val_mask, test_mask)\n",
        "    print(f\"test loss: {loss:.5f}, Val acc: {val_acc:.5f}, Test acc: {test_acc:.5f}\")\n",
        "\n",
        "# attn 200epoch 1min test loss: 0.00641, Val acc: 0.76000, Test acc: 0.74700\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m-1wJk4LZFg",
        "outputId": "75473b9b-1bed-4a8d-94f3-0dcaf34e777a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2708, 2708]) torch.Size([8137])\n",
            "torch.Size([2708, 2708]) torch.Size([8137])\n",
            "torch.Size([2708, 2708]) torch.Size([8137])\n",
            "torch.Size([2708, 2708]) torch.Size([8137])\n",
            "torch.Size([2708, 7]) torch.Size([2708, 2708]) torch.Size([2708, 1433])\n",
            "test loss: 1.93871, Val acc: 0.14000, Test acc: 0.19300\n",
            "torch.Size([2708, 2708]) torch.Size([8137])\n",
            "torch.Size([2708, 2708]) torch.Size([8137])\n",
            "torch.Size([2708, 2708]) torch.Size([8137])\n",
            "torch.Size([2708, 2708]) torch.Size([8137])\n",
            "torch.Size([2708, 7]) torch.Size([2708, 2708]) torch.Size([2708, 1433])\n",
            "test loss: 1.84748, Val acc: 0.30000, Test acc: 0.28300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title old og hg attn\n",
        "Hypergraph Convolution and Hypergraph Attention\n",
        "(https://arxiv.org/pdf/1901.08150.pdf).\n",
        "import argparse\n",
        "import dgl.sparse as dglsp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "from dgl.data import CoraGraphDataset\n",
        "def accuracy(yhat, y): return (yhat.argmax(1) == y).type(torch.float).sum().item()/y.shape[0]\n",
        "\n",
        "\n",
        "def hypergraph_laplacian(H):\n",
        "    ###########################################################\n",
        "    # (HIGHLIGHT) Compute the Laplacian with Sparse Matrix API\n",
        "    ###########################################################\n",
        "    d_V = H.sum(1)  # node degree\n",
        "    d_E = H.sum(0)  # edge degree\n",
        "    n_edges = d_E.shape[0]\n",
        "    D_V_invsqrt = dglsp.diag(d_V**-0.5)  # D_V ** (-1/2)\n",
        "    D_E_inv = dglsp.diag(d_E**-1)  # D_E ** (-1)\n",
        "    W = dglsp.identity((n_edges, n_edges))\n",
        "    return D_V_invsqrt @ H @ W @ D_E_inv @ H.T @ D_V_invsqrt\n",
        "\n",
        "\n",
        "class HypergraphAttention(nn.Module):\n",
        "    \"\"\"Hypergraph Attention module as in the paper\n",
        "    `Hypergraph Convolution and Hypergraph Attention\n",
        "    <https://arxiv.org/pdf/1901.08150.pdf>`_.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.P = nn.Linear(in_size, out_size)\n",
        "        self.a = nn.Linear(2 * out_size, 1)\n",
        "\n",
        "    def forward(self, H, X, X_edges):\n",
        "        Z = self.P(X)\n",
        "        Z_edges = self.P(X_edges)\n",
        "        # print(\"H\",H.shape) # 2708, 2708\n",
        "        # print(\"H.row,H.col\",H.row.shape,H.col.shape) # H.row,H.col tensor([   0,    0,    0,  ..., 2707, 2707, 2707]) tensor([   0,  633, 1862,  ..., 1473, 2706, 2707]) # [13264], [13264]\n",
        "        # print(\"Z[H.row], Z_edges[H.col]\",Z[H.row], Z_edges[H.col].shape) # [13264, 16], [13264, 16]\n",
        "        print(Z,Z_edges.shape) # [2708, 16], [2708, 16]\n",
        "\n",
        "        sim = self.a(torch.cat([Z[H.row], Z_edges[H.col]], 1))\n",
        "        sim = F.leaky_relu(sim, 0.2).squeeze(1)\n",
        "        # Reassign the hypergraph new weights.\n",
        "        H_att = dglsp.val_like(H, sim)\n",
        "        H_att = H_att.softmax()\n",
        "        return hypergraph_laplacian(H_att) @ Z\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_size, out_size, hidden_size=16):\n",
        "        super().__init__()\n",
        "        self.layer1 = HypergraphAttention(in_size, hidden_size)\n",
        "        self.layer2 = HypergraphAttention(hidden_size, out_size)\n",
        "\n",
        "    def forward(self, H, X):\n",
        "        Z = self.layer1(H, X, X)\n",
        "        Z = F.elu(Z)\n",
        "        Z = self.layer2(H, Z, Z)\n",
        "        return Z\n",
        "\n",
        "\n",
        "def train(model, optimizer, H, X, Y, train_mask):\n",
        "    model.train()\n",
        "    Y_hat = model(H, X)\n",
        "    loss = F.cross_entropy(Y_hat[train_mask], Y[train_mask])\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def evaluate(model, H, X, Y, val_mask, test_mask, num_classes):\n",
        "    model.eval()\n",
        "    Y_hat = model(H, X)\n",
        "    val_acc = accuracy(Y_hat[val_mask], Y[val_mask])\n",
        "    test_acc = accuracy(Y_hat[test_mask], Y[test_mask])\n",
        "    return val_acc, test_acc\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    dataset = CoraGraphDataset()\n",
        "    graph = dataset[0]\n",
        "    indices = torch.stack(graph.edges())\n",
        "    H = dglsp.spmatrix(indices)\n",
        "    H = H + dglsp.identity(H.shape)\n",
        "    X = graph.ndata[\"feat\"]\n",
        "    Y = graph.ndata[\"label\"]\n",
        "    train_mask = graph.ndata[\"train_mask\"]\n",
        "    val_mask = graph.ndata[\"val_mask\"]\n",
        "    test_mask = graph.ndata[\"test_mask\"]\n",
        "    return H, X, Y, dataset.num_classes, train_mask, val_mask, test_mask\n",
        "\n",
        "\n",
        "H, X, Y, num_classes, train_mask, val_mask, test_mask = load_data()\n",
        "model = Net(X.shape[1], num_classes)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "with tqdm.trange(2) as tq:\n",
        "    for epoch in tq:\n",
        "        loss = train(model, optimizer, H, X, Y, train_mask)\n",
        "        val_acc, test_acc = evaluate(\n",
        "            model, H, X, Y, val_mask, test_mask, num_classes\n",
        "        )\n",
        "        tq.set_postfix(\n",
        "            {\n",
        "                \"Loss\": f\"{loss:.5f}\",\n",
        "                \"Val acc\": f\"{val_acc:.5f}\",\n",
        "                \"Test acc\": f\"{test_acc:.5f}\",\n",
        "            },\n",
        "            refresh=False,\n",
        "        )\n",
        "\n",
        "print(f\"Test acc: {test_acc:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sTRlHcQtEFXF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}